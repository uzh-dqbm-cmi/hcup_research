{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we define relevant directories\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "# project directory\n",
    "project_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "# src directory\n",
    "src_dir = os.path.join(project_dir, 'src')\n",
    "sys.path.insert(0, src_dir)\n",
    "dataset_dir = os.path.join(project_dir, \"dataset\")\n",
    "print(\"dataset_dir: \", dataset_dir)\n",
    "print(\"project_dir: \", project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from explore_hcupdata import *\n",
    "from utilities import create_directory, ReaderWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read relevant data\n",
    "CONT_COLS = ReaderWriter.read_data(os.path.join(dataset_dir, 'continuous_features.pkl'))\n",
    "COL_FEATURES = ReaderWriter.read_data(os.path.join(dataset_dir, 'col_features.pkl'))\n",
    "feat_label = ReaderWriter.read_data(os.path.join(dataset_dir, 'feat_label.pkl'))\n",
    "code_feat = ReaderWriter.read_data(os.path.join(dataset_dir, 'code_feat.pkl'))\n",
    "datasplit = ReaderWriter.read_data(os.path.join(dataset_dir, 'datasplit.pkl'))\n",
    "traj_info = ReaderWriter.read_data(os.path.join(dataset_dir, 'traj_info.pkl'))\n",
    "fsample = ReaderWriter.read_data(os.path.join(dataset_dir, 'fsample.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PADDSYMB_INDX = 1000\n",
    "\n",
    "class PatientDataTensorMemmap:\n",
    "    def __init__(self, idx_mapper, dtype, paddsymbol):\n",
    "        self.idx_mapper = idx_mapper\n",
    "        self.idx_mapper_inverse = {numcode:pid for pid, numcode in self.idx_mapper.items()}\n",
    "        self.dsettype = dtype\n",
    "        self.paddsymbol = paddsymbol\n",
    "        \n",
    "    def memmap_arrays(self, X_tensor, Y_tensor, E_tensor, T_tensor, fpath):\n",
    "        # create a memmap numpy arrays\n",
    "        tensor_info = ['seqtensor_info', 'labeltensor_info', \n",
    "                       'indexeventtensor_info', 'seqlentensor_info']\n",
    "        arrays = [X_tensor, Y_tensor, E_tensor, T_tensor]\n",
    "        array_names = ['seq_tensor', 'label_tensor', 'indexevent_tensor', 'seqlen_tensor']\n",
    "        for i, (arr, arr_name) in enumerate(zip(arrays, array_names)):\n",
    "            tmparr = np.memmap(os.path.join(fpath, arr_name+'.dat'), dtype=arr.dtype, mode='w+', shape=arr.shape)\n",
    "            tmparr[:] = arr[:]\n",
    "            setattr(self, tensor_info[i], (arr.dtype, arr.shape))\n",
    "        self.num_samples = X_tensor.shape[0]\n",
    "        self.input_dim = X_tensor.shape[-1]\n",
    "         \n",
    "    def read_fromdisk(self, fpath, memmap = True):\n",
    "        # to refactor this function/process\n",
    "        # due to issues with multiprocessing and size of seq_tensor\n",
    "        # loading seq_tensor is delayed until it is called within the spawned child processes\n",
    "        array_names = ['seq_tensor', 'label_tensor', 'indexevent_tensor', 'seqlen_tensor']\n",
    "        tensor_info = [self.seqtensor_info, self.labeltensor_info, \n",
    "                       self.indexeventtensor_info, self.seqlentensor_info]\n",
    "        for arr_info, arr_name in zip(tensor_info, array_names):\n",
    "            arr = np.memmap(os.path.join(fpath, arr_name+'.dat'), dtype=arr_info[0], mode = 'r', shape=arr_info[1])\n",
    "            if(not memmap):\n",
    "                arr = np.asarray(arr)\n",
    "            setattr(self, arr_name, arr)\n",
    "        if(hasattr(self, 'fpath')):\n",
    "            del self.fpath\n",
    "\n",
    "def build_numpy_tensor(gdf, tensor, colnames, pid_mapper):\n",
    "    patient_id = pid_mapper[gdf.iloc[0]['nrd_visitlink']]\n",
    "    tensor[patient_id, :len(gdf), :] = gdf[colnames]\n",
    "    \n",
    "def generate_patientdataset_tensor(sample, traj_info, colfeatures, sample_type, fpath):\n",
    "    y_colname = 'allcause_readmit'\n",
    "    idx = sample['nrd_visitlink'].unique()\n",
    "    target_idx = traj_info['nrd_visitlink'].isin(idx)\n",
    "#     max_seqlen = traj_info.loc[target_idx,'seq_len'].max()\n",
    "    max_seqlen = traj_info['seq_len'].max() # to keep all datasets with uniform Time length\n",
    "    N = len(idx) # total number of samples (i.e patients)\n",
    "    input_dim = len(colfeatures)\n",
    "    pid_mapper = {vstlink:int(code) for code, vstlink in enumerate(list(idx))}\n",
    "    # build sequence tensor\n",
    "    X_tensor = np.zeros((N, max_seqlen, input_dim))\n",
    "    sample.groupby(['nrd_visitlink']).apply(build_numpy_tensor, X_tensor, colfeatures, pid_mapper)\n",
    "    Y_tensor = np.zeros((N, max_seqlen, 1)) + PADDSYMB_INDX\n",
    "    # build label tensor\n",
    "    sample.groupby(['nrd_visitlink']).apply(build_numpy_tensor, Y_tensor, [y_colname], pid_mapper)\n",
    "    # build the index event tensor\n",
    "    E_tensor = np.zeros((N, max_seqlen, 1)) + PADDSYMB_INDX\n",
    "    sample.groupby(['nrd_visitlink']).apply(build_numpy_tensor, E_tensor, ['index_event'], pid_mapper)\n",
    "    # build sequence length tensor\n",
    "    temp = traj_info.loc[target_idx, ['nrd_visitlink', 'seq_len']]\n",
    "    T_tensor = np.ones(N, dtype='uint8')*-1\n",
    "    T_idx =np.vectorize(pid_mapper.get)(temp['nrd_visitlink'])\n",
    "    T_tensor[T_idx] = temp['seq_len'].values \n",
    "    patient_data = PatientDataTensorMemmap(pid_mapper, sample_type, PADDSYMB_INDX)\n",
    "    patient_data.memmap_arrays(X_tensor, Y_tensor, E_tensor, T_tensor, fpath)\n",
    "    return(patient_data)\n",
    "\n",
    "def generate_classweights(fold_name, train_idx, option):\n",
    "    train_sample = traj_info.loc[traj_info['nrd_visitlink'].isin(train_idx)]\n",
    "    if(option == 'all_indx'): # use all index events in the computation\n",
    "        nsamples = train_sample['run_num_indxevents'].sum()\n",
    "        n_one = train_sample['count_allcausereadmit'].sum()\n",
    "    elif(option == 'last_indx'): # use last index event in the computation\n",
    "        nsamples = train_sample.shape[0]\n",
    "        n_one = train_sample['allcause_readmit'].sum()\n",
    "    elif(option == 'all'): # use all events\n",
    "        nsamples = train_sample['run_num_events'].sum()\n",
    "        n_one = train_sample['count_allcausereadmit'].sum()\n",
    "    n_zero = nsamples - n_one\n",
    "    print(\"number of samples: \", nsamples)\n",
    "    print(\"number of zeros: \", n_zero)\n",
    "    print(\"number of ones: \", n_one)\n",
    "    print(\"readmit rate: \", n_one/nsamples)\n",
    "    w_zero = nsamples/(2*n_zero)\n",
    "    w_one = nsamples/(2*n_one)\n",
    "    print(\"fold: \", fold_name)\n",
    "    print(\"w_zero:{}, w_one:{}\".format(w_zero, w_one))\n",
    "    print(\"-\"*10)\n",
    "    return((w_zero, w_one))\n",
    "\n",
    "def generate_datasetfolds(datafolds, fsample, dataset_dir, norm_option):\n",
    "    dsettypes = ('train', 'validation', 'test')\n",
    "    for fold_name in datafolds:\n",
    "        print(\"fold \", fold_name)\n",
    "        cdir = create_directory(\"{}_{}\".format(fold_name, norm_option), dataset_dir)\n",
    "        target_dsets = {}\n",
    "        # datafolds[fold_name] is a tuple of (train, validation, test) indices/visitlinks\n",
    "        for i, target_idx in enumerate(datafolds[fold_name]): \n",
    "            dset = fsample.loc[fsample['nrd_visitlink'].isin(target_idx)].copy()\n",
    "            print(dsettypes[i] + \" dataset\")\n",
    "            print(len(dset))\n",
    "            print()\n",
    "            if(norm_option != 'none'):\n",
    "                apply_normalization(dset, \n",
    "                                    CONT_COLS, \n",
    "                                    ReaderWriter.read_data(os.path.join(cdir, \"{}_info.pkl\".format(norm_option))))\n",
    "            pdt_path = create_directory(dsettypes[i]+\"_pdtm\", cdir)\n",
    "            pdt = generate_patientdataset_tensor(dset, traj_info, COL_FEATURES, dsettypes[i], pdt_path)\n",
    "            # pickle data\n",
    "            fpathname = os.path.join(pdt_path, 'pdtm_object.pkl')\n",
    "            ReaderWriter.dump_data(pdt, fpathname)\n",
    "            target_dsets[pdt.dsettype] = fpathname\n",
    "            if(dsettypes[i] == 'train'):\n",
    "                for classweight_option in ('last_indx',): # get the class weights\n",
    "                    class_weights = generate_classweights(fold_name, target_idx, classweight_option)\n",
    "                    ReaderWriter.dump_data(class_weights, os.path.join(pdt_path, \"classweights_\" + classweight_option + \".pkl\"))\n",
    "        # pickle the pointer dictionary on disk\n",
    "        ReaderWriter.dump_data(target_dsets, os.path.join(cdir, 'dataset_tuple.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate subset fold for hyperparam tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing option with dataset directory\n",
    "norm_options = ('none', )\n",
    "dataset_neural_dir = create_directory('dataset_neural', project_dir)\n",
    "\n",
    "# build the subset dataset for hyperparameter optimization\n",
    "train_idx, val_idx = get_datasubset(datasplit, traj_info, 0.3)\n",
    "subsetfolds = {'subsetfold':(train_idx,  val_idx)}\n",
    "# generate normalizer\n",
    "# generate_normalizers(subsetfolds, fsample, dataset_neural_dir, CONT_COLS, normalize_options = norm_options)\n",
    "generate_datasetfolds(subsetfolds, fsample, dataset_neural_dir, norm_options[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate five folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datafolds -> fold_id:(train_idx, val_idx, test_idx)\n",
    "datafolds = get_datafolds(datasplit, traj_info, 0.2)\n",
    "# generate normalizer\n",
    "# generate_normalizers(datafolds, fsample, dataset_neural_dir, CONT_COLS, normalize_options = norm_options)\n",
    "generate_datasetfolds(datafolds, fsample, dataset_neural_dir, norm_options[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
