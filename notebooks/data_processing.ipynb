{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we define relevant directories\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "# project directory\n",
    "project_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "# src directory (below)\n",
    "src_dir = os.path.join(project_dir, 'src')\n",
    "sys.path.insert(0, src_dir)\n",
    "# get the tutorials dir\n",
    "notebooks_dir = os.path.join(project_dir, 'notebooks')\n",
    "dataset_dir = os.path.join(project_dir, \"dataset\")\n",
    "print(\"dataset_dir: \", dataset_dir)\n",
    "print(\"project_dir: \", project_dir)\n",
    "print(\"notebooks_dir: \", notebooks_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from explore_hcupdata import *\n",
    "from utilities import create_directory, ReaderWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the NRD_Core dataset\n",
    "nrd_core = load_nrd('Core', dataset_dir)\n",
    "target_dir = create_directory('missing_analysis', project_dir)\n",
    "report_missing(nrd_core, os.path.join(target_dir, 'check_missing_report_before.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing data\n",
    "fill_missing(nrd_core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check again the status of missing -- sanity check\n",
    "check_missing(nrd_core, os.path.join(target_dir, 'check_missing_report_after.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle the cleaned/updated NRD_CORE\n",
    "nrd_core.to_pickle(os.path.join(dataset_dir, 'NRD_CORE_2013_cleaned.pkl'))\n",
    "del target_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning/processing dataset to obtain target population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_visitlinks(df):\n",
    "    primary_cond = df['dxccs1'] == 108 # congestive heart failure primary diagnosis\n",
    "    age_cond = df['age'] >= 18\n",
    "    visitlinks_A = set(df.loc[(primary_cond & age_cond), 'nrd_visitlink'].unique().tolist())\n",
    "\n",
    "    # create a pseudo discharge date if it is not there\n",
    "    if('pseudoddate' not in df):\n",
    "        print(\"creating pseudo discharge date\")\n",
    "        df['pseudoddate'] = df['nrd_daystoevent'] + df['los']\n",
    "\n",
    "    # this condition will check simultaneously the nrd_daystoevent and los\n",
    "    # check patients with invlaide discharge dates\n",
    "    cond_pseudoddate = pd.isnull(df['pseudoddate'])\n",
    "    visitlinks_B = set(df.loc[(cond_pseudoddate), 'nrd_visitlink'].unique().tolist())\n",
    "\n",
    "    # visitlinks of patients that are valid\n",
    "    visitlinks_C = visitlinks_A - visitlinks_B\n",
    "    return(visitlinks_C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_visitlinks = list(get_valid_visitlinks(nrd_core))\n",
    "sample_nrd = nrd_core.loc[nrd_core['nrd_visitlink'].isin(valid_visitlinks)].copy()\n",
    "sample_nrd.sort_values(by=['nrd_visitlink', 'pseudoddate'], inplace=True)\n",
    "# target cols to show for now\n",
    "show_cols = ['nrd_visitlink', 'age', 'nrd_daystoevent','los','pseudoddate', 'dxccs1', 'dxccs2', 'dmonth']\n",
    "del valid_visitlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the number of days to readmission\n",
    "def compute_readmit(gdf):\n",
    "    res = gdf['nrd_daystoevent'].shift(-1) - gdf['pseudoddate']\n",
    "    return(res)\n",
    "s = sample_nrd.groupby(['nrd_visitlink']).apply(compute_readmit)\n",
    "s = s.to_frame().reset_index()\n",
    "s.set_index('level_1', inplace=True)\n",
    "sample_nrd['readmit'] = s[0]\n",
    "sample_nrd[show_cols+['readmit']].head()\n",
    "# clean temp vars\n",
    "del s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check the delta_t denoted by readmit column if it is positive\n",
    "outliers = sample_nrd.loc[sample_nrd['readmit']<0, show_cols + ['readmit']]\n",
    "print(outliers)\n",
    "print(\"-\"*30)\n",
    "# after inspection there are 32 outliers having future events in which \n",
    "# they are admitted before the discharge of previous events in their trajectory \n",
    "print(\"number of outliers \", len(outliers))\n",
    "print()\n",
    "# explore why this argument is valid by inspecting their full trajectory -- using 5 of these outliers as an example\n",
    "outliers_vstlink = outliers['nrd_visitlink']\n",
    "print(sample_nrd.loc[sample_nrd['nrd_visitlink'].isin(outliers_vstlink[:5]), show_cols + ['readmit']])\n",
    "print()\n",
    "# drop these outliers as they have inconsistent trajectory\n",
    "outliers_indx = sample_nrd.loc[sample_nrd['nrd_visitlink'].isin(outliers_vstlink)].index\n",
    "print(\"number of events correpsonding to outlier patients: \", len(outliers_indx))\n",
    "sample_nrd.drop(outliers_indx, axis=0, inplace=True)\n",
    "# clean temp vars\n",
    "del outliers, outliers_vstlink, outliers_indx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_age(gdf):\n",
    "    age_diff = gdf['age'].shift(-1) - gdf['age']\n",
    "    age_diff_sum = age_diff.sum()\n",
    "    # given it is one-year data, the difference in age should be at most 1\n",
    "    flag = age_diff_sum >= 0 and age_diff_sum <=1 \n",
    "    return(not flag)\n",
    "s = sample_nrd.groupby(['nrd_visitlink']).apply(check_age)\n",
    "\n",
    "# check outliers due to age inconsistencies\n",
    "ageissues_vstlink = s[s==True].index.tolist()\n",
    "print(\"patients visitlink with age issues:\\n\", ageissues_vstlink)\n",
    "print(sample_nrd.loc[sample_nrd['nrd_visitlink'].isin(ageissues_vstlink), show_cols + ['readmit']])\n",
    "# drop age outlier patients\n",
    "ageissues_indx = sample_nrd.loc[sample_nrd['nrd_visitlink'].isin(ageissues_vstlink)].index\n",
    "sample_nrd.drop(ageissues_indx, axis=0, inplace=True)\n",
    "# sanity check if patients with inconsistent age are still there\n",
    "print(sample_nrd.loc[sample_nrd['nrd_visitlink'].isin(ageissues_vstlink), show_cols + ['readmit']])\n",
    "del s, ageissues_vstlink, ageissues_indx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gender(gdf):\n",
    "    gender_diff = gdf['female'].shift(-1) - gdf['female']\n",
    "    gender_diff_sum = gender_diff.sum()\n",
    "    flag = gender_diff_sum == 0 # consistent gender coding\n",
    "    return(not flag)\n",
    "s = sample_nrd.groupby(['nrd_visitlink']).apply(check_gender)\n",
    "\n",
    "# check outliers due to gender inconsistencies\n",
    "genderissues_vstlink = s[s==True].index.tolist()\n",
    "print(\"patients visitlink with inconsistent gender coding:\\n\", genderissues_vstlink)\n",
    "print(sample_nrd.loc[sample_nrd['nrd_visitlink'].isin(genderissues_vstlink), show_cols + ['readmit']])\n",
    "# drop \n",
    "genderissues_indx = sample_nrd.loc[sample_nrd['nrd_visitlink'].isin(genderissues_vstlink)].index\n",
    "sample_nrd.drop(genderissues_indx, axis=0, inplace=True)\n",
    "# sanity check\n",
    "print(sample_nrd.loc[sample_nrd['nrd_visitlink'].isin(genderissues_vstlink), show_cols + ['readmit']])\n",
    "del s, genderissues_vstlink, genderissues_indx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create col for index event (i.e. when it the primary diagnosis of the event is congestive heart failure)\n",
    "sample_nrd['index_event'] = 0\n",
    "sample_nrd.loc[sample_nrd['dxccs1'] == 108, 'index_event'] = 1\n",
    "# create col for 30 days all cause readmission after index event (i.e. target outcome variable)\n",
    "sample_nrd['allcause_readmit'] = 0\n",
    "sample_nrd.loc[((sample_nrd['readmit'] <= 30) & (sample_nrd['index_event'] == 1)), 'allcause_readmit'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_nrd['rindx'] = sample_nrd.index # preserve original index in a new column\n",
    "# check died events (if existing) should be the last events\n",
    "diedevent_indx = sample_nrd.loc[sample_nrd['died'] == 1].index\n",
    "levent_indx = sample_nrd.groupby(['nrd_visitlink']).nth(-1)['rindx']\n",
    "# it is confirmed that registered died events are last events\n",
    "print(diedevent_indx.shape)\n",
    "print(levent_indx.shape)\n",
    "diff = set(diedevent_indx.tolist()) - set(levent_indx.tolist())\n",
    "print(len(diff))\n",
    "print(diff)\n",
    "# remove died events from trajectory -- they are not counted as index events by definition\n",
    "sample_nrd.drop(diedevent_indx, axis=0, inplace=True)\n",
    "del diedevent_indx, levent_indx, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update column index\n",
    "sample_nrd['rindx'] = sample_nrd.index\n",
    "# get all events in December and drop them as we cannot establish for all patients the readmission outcome for events occuring in December\n",
    "# we are considering all events from Jan to November which we can establish their readmission outcome\n",
    "december_eventsdf = sample_nrd.loc[sample_nrd['dmonth'] == 12]\n",
    "print(december_eventsdf[show_cols + ['readmit', 'allcause_readmit']])\n",
    "december_eventsindx = december_eventsdf.index\n",
    "sample_nrd.drop(december_eventsindx, axis=0, inplace=True)\n",
    "print(sample_nrd.loc[sample_nrd['dmonth'] == 12]) # confirm deletion\n",
    "del december_eventsdf, december_eventsindx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute readmission: 30-day readmission/index events\n",
    "num_hf_indxevents = sample_nrd['index_event'].sum()\n",
    "# print(\"HF readmission rate: \", sample_nrd['hf_readmit'].sum()/num_hf_indxevents)\n",
    "print(\"30-day all-cause readmission rate: \", sample_nrd['allcause_readmit'].sum()/num_hf_indxevents)\n",
    "# pickle the dataset for now\n",
    "sample_nrd.to_pickle(os.path.join(dataset_dir, 'workingsample_HF_v1.pkl'))\n",
    "del num_hf_indxevents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_nonhf_patients(sample_nrd):\n",
    "    \"\"\"in-place drop of patients who do not have index events after processing\"\"\"\n",
    "    cond = sample_nrd['dxccs1'] == 108\n",
    "    hf_vstlinks = set(sample_nrd.loc[cond, 'nrd_visitlink'].unique())\n",
    "    all_vstlinks = set(sample_nrd['nrd_visitlink'].unique())\n",
    "    # patients who do not have index events \n",
    "    # (i.e. no heart failure events in their trajectory after the applied processing)\n",
    "    diff = all_vstlinks - hf_vstlinks\n",
    "    print(\"number of patients with no heart failure event in their trajectory: \", len(diff))\n",
    "    print()\n",
    "    sample_nrd.drop(sample_nrd.loc[sample_nrd['nrd_visitlink'].isin(diff)].index, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsample = sample_nrd\n",
    "# updated show columns\n",
    "show_cols = ['nrd_visitlink', 'nrd_daystoevent','los','pseudoddate','dmonth', 'readmit', 'allcause_readmit', 'dxccs1', 'dxccs2']\n",
    "filter_nonhf_patients(fsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep trajectories until the last HF event (i.e. inclusive) in a specified month\n",
    "# warning: this cell might take some time to compute ~20 mins\n",
    "def keep_traj_uptoHF(sample_nrd):\n",
    "    def _keep_traj_uptoHF(gdf):\n",
    "        cond = (gdf['dxccs1'] == 108)\n",
    "        l_indx = np.where(cond)[0][-1] # last index event index\n",
    "        gdf_index = gdf.index # indices of all events for the current patient\n",
    "        return(gdf.loc[gdf_index[:l_indx+1]])\n",
    "    s = sample_nrd.groupby(['nrd_visitlink']).apply(_keep_traj_uptoHF)\n",
    "    s.reset_index(level=0, drop=True, inplace=True)\n",
    "    return(s)\n",
    "\n",
    "def compute_readmission_rate(fsample):\n",
    "    # explore the readmission outcome for different minimum number of event in trajectory\n",
    "    patients_g = fsample.groupby(['nrd_visitlink'])\n",
    "    gsizes = patients_g.size()\n",
    "    seqlens = gsizes.value_counts().index.tolist()\n",
    "    for i in seqlens:\n",
    "        target_p = gsizes[gsizes >=i].index.tolist()\n",
    "        num_patients = len(target_p)\n",
    "        print(\"number of patients with number of events >= to {0}: {1}\".format(i, num_patients))\n",
    "        # compute readmission \n",
    "        tmp = fsample.loc[fsample['nrd_visitlink'].isin(target_p)]\n",
    "        num_patients = tmp['nrd_visitlink'].unique().shape[0]\n",
    "        print(\"total number of patients: \", num_patients)\n",
    "        num_hf_indxevents = (tmp['dxccs1'] == 108).sum()\n",
    "        print(\"number of index events: \", num_hf_indxevents)\n",
    "        allevent_readmit = tmp['allcause_readmit'].sum()\n",
    "        print(\"number of 30-day all-cause readmissions: \", allevent_readmit)\n",
    "        print(\"30-day all-cause readmission: \", allevent_readmit/num_hf_indxevents)\n",
    "        lastevent_readmit = tmp.groupby('nrd_visitlink')['allcause_readmit'].nth(-1).sum()\n",
    "        print(\"last event readmission: \", lastevent_readmit/num_patients)        \n",
    "        print(\"30-day all-cause readmission for all event except the last event: \", (allevent_readmit-lastevent_readmit)/(num_hf_indxevents-num_patients))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_diagproc_df(fsample, code_label_df, colname, qthreshold):\n",
    "    if(colname == 'dxccs'):\n",
    "        total_cols = 25\n",
    "        name = 'diagnosis'\n",
    "    elif(colname == 'prccs'):\n",
    "        total_cols = 15\n",
    "        name = 'procedures'\n",
    "    cols_name = [\"{}{}\".format(colname, i) for i in range(1,total_cols+1)]\n",
    "    target_df = pd.melt(fsample[cols_name], value_vars=cols_name, var_name=colname+'_rank', value_name=colname+'_val')\n",
    "\n",
    "    # target_diag = [108.0, 249.0, 131.0, 107.0, 106.0, 105.0, 157.0, 55.0]\n",
    "    tmp = target_df[colname+'_val'].value_counts()\n",
    "    top_categs_df = tmp[tmp > tmp.quantile(qthreshold)].dropna()\n",
    "    print(top_categs_df.shape)\n",
    "    print(\"number of {} in top {}%: {}\".format(name, (1-qthreshold)*100, len(top_categs_df)))\n",
    "    print(\"-\"*50)\n",
    "    fig, ax = plt.subplots(figsize=(16, 9))\n",
    "    top_categs_df.plot(kind='bar') \n",
    "    # build a dataframe from top label categories series\n",
    "    top_categs_df = top_categs_df.to_frame()\n",
    "    top_categs_df.index = top_categs_df.index.astype('int')\n",
    "    top_categs_df.reset_index(inplace=True)\n",
    "    top_categs_df.columns = ['ccs_code', 'count']\n",
    "    top_categs_df['ccs_code'] = top_categs_df['ccs_code'].astype('str')\n",
    "    # merge to get the label of top diagnosis\n",
    "    # the keys should have the same type\n",
    "    top_categs_df = pd.merge(left=top_categs_df, right=code_label_df, how='left', on=['ccs_code'])\n",
    "    return(top_categs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_onehot(fsample, targetcol_prefix, labelspace_lst, num_targetcols, prefix):\n",
    "    z = pd.DataFrame()\n",
    "    for i in range(1, num_targetcols+1):\n",
    "        targetcol_name = '{}{}'.format(targetcol_prefix, i)\n",
    "        flag= fsample[targetcol_name].isnull().any() # check if nan values are there\n",
    "        fsample[\"$categ_col$\"] = fsample[targetcol_name]\n",
    "        fsample[\"$categ_col$\"] = fsample[\"$categ_col$\"].astype('category',categories=labelspace_lst)\n",
    "        onehot_df = pd.get_dummies(fsample[\"$categ_col$\"],prefix='{}{}'.format(prefix, i), dummy_na=flag)\n",
    "        z=pd.concat([z, onehot_df], axis=1)\n",
    "    del fsample[\"$categ_col$\"] # remove the temporary categorical variable\n",
    "    return(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate missingness in the first, second, and third diagnosis and procedures fields/cols\n",
    "for colprefix in ('dxccs', 'prccs'):\n",
    "    for i in range(1,4):\n",
    "        num_null = fsample['{}{}'.format(colprefix, i)].isnull().sum()\n",
    "        print(\"the % of missing values in {}{} is {}\".format(colprefix, i, num_null/fsample.shape[0] * 100))\n",
    "    del num_null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate features/computed variables from diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varcoding_dir = create_directory('variables', project_dir)\n",
    "# diagnosis codes and associated labels\n",
    "dxlabel = pd.read_csv(os.path.join(varcoding_dir, 'dxlabel 2015.csv'))\n",
    "dxlabel.columns = ['ccs_code', 'ccs_label']\n",
    "top_diagn_df = get_top_diagproc_df(fsample, dxlabel, 'dxccs', qthreshold=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build one-hot encoding for first, second and third diagnosis\n",
    "top_diagn_lst = top_diagn_df['ccs_code'].astype('float32').tolist()\n",
    "onehot_df = make_onehot(fsample, \"dxccs\", top_diagn_lst, 3, \"diagn\")\n",
    "# concat with fsample\n",
    "fsample = pd.concat([fsample, onehot_df], axis=1)\n",
    "del top_diagn_lst, onehot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute new variables representing counts of every diagnosis category recorded in the 25 diagnosis columns\n",
    "diagncols_name = [\"dxccs{}\".format(i) for i in range(1,26)]\n",
    "# we need to cast the ccs_code to float before looking it up\n",
    "top_diagn_lst = top_diagn_df['ccs_code'].astype('float32').tolist()\n",
    "for top_diagn in top_diagn_lst:\n",
    "    cond = fsample[diagncols_name] == top_diagn\n",
    "    print(cond.head())\n",
    "    fsample['diagncount_{}'.format(top_diagn)] = cond.sum(axis=1)\n",
    "    print(\"-\"*40)\n",
    "del diagncols_name, top_diagn_lst, cond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate features/computed variables from procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# procedures codes with associated labels\n",
    "prlabel = pd.read_csv(os.path.join(varcoding_dir, 'prlabel 2014 cleaned.csv'), sep=',')\n",
    "prlabel.columns = ['ccs_code', 'ccs_label']\n",
    "top_proc_df = get_top_diagproc_df(fsample, prlabel, 'prccs', qthreshold=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build one-hot encoding for first, second and third procedures\n",
    "top_proc_lst = top_proc_df['ccs_code'].astype('float32').tolist()\n",
    "onehot_df = make_onehot(fsample, \"prccs\", top_proc_lst, 3, \"proc\")\n",
    "# concat with fsample\n",
    "fsample = pd.concat([fsample, onehot_df], axis=1)\n",
    "del onehot_df, top_proc_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute new variables representing counts of every procedure category recorded in the 15 procedures columns\n",
    "proccols_name = [\"prccs{}\".format(i) for i in range(1,16)]\n",
    "# produce the count using all the procedure categories\n",
    "top_proc_lst = top_proc_df['ccs_code'].astype('float32').tolist()\n",
    "for top_proccol in top_proc_lst:\n",
    "    cond = fsample[proccols_name] == top_proccol\n",
    "    print(cond.head())\n",
    "    fsample['proccount_{}'.format(top_proccol)] = cond.sum(axis=1)\n",
    "    print(\"-\"*40)\n",
    "del proccols_name, top_proc_lst, cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_deltat(gdf):\n",
    "    gdf['deltat']=gdf['readmit'].shift(1)\n",
    "    return(gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create delta_t variable\n",
    "fsample = fsample.groupby(['nrd_visitlink']).apply(create_deltat)\n",
    "fsample['deltat'].fillna(0, inplace=True)\n",
    "# reupdate the dtype of the variables\n",
    "convert_dtypes(fsample)\n",
    "# dump data checkpoint\n",
    "ReaderWriter.dump_data(fsample, os.path.join(dataset_dir, 'workingsample_HF_ckpt1.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['e_ccs{}'.format(i) for i in range(1,5)] + ['ecode{}'.format(i) for i in range(1,5)]\n",
    "check_percent_missing(fsample, cols)\n",
    "del cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build one-hot encoding for first ecodes\n",
    "ecodes = set([])\n",
    "for i in range(1, 5):\n",
    "    ecodes = ecodes.union(set(fsample['e_ccs{}'.format(i)].value_counts().index))\n",
    "ecodes_lst = list(ecodes)\n",
    "onehot_df = make_onehot(fsample, \"e_ccs\", ecodes_lst, 1, \"ecode\")\n",
    "fsample = pd.concat([fsample, onehot_df], axis=1)\n",
    "\n",
    "# setup the ecode injuries code counts\n",
    "for ecode_categ in ecodes_lst:\n",
    "    cond = fsample[[\"e_ccs{}\".format(i) for i in range(1, 5)]] == ecode_categ\n",
    "    print(cond.head())\n",
    "    fsample['ecodecount_{}'.format(ecode_categ)] = cond.sum(axis=1)\n",
    "    print(\"-\"*40)\n",
    "del ecodes, ecodes_lst, onehot_df, cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_runningstats(gdf):\n",
    "    gdf['run_num_indxevents'] = gdf['index_event'].cumsum()\n",
    "    gdf['run_num_events'] = np.repeat(1, gdf.shape[0]).cumsum()\n",
    "    gdf['run_percent_indexevents'] = gdf['run_num_indxevents']/gdf['run_num_events']\n",
    "    gdf.reset_index(inplace=True)\n",
    "    one_arr = gdf.loc[gdf['index_event'] == 1].index\n",
    "    indx = gdf.index\n",
    "    gdf['numdays_from_lastindex'] = -1\n",
    "    if(len(one_arr)>1):\n",
    "        for i in range(len(one_arr)-1):\n",
    "            anchor_1 = one_arr[i]\n",
    "            anchor_2 = one_arr[i+1]\n",
    "            cond = (indx>anchor_1) & (indx<=anchor_2)\n",
    "            gdf.loc[indx[cond],'numdays_from_lastindex'] = gdf.loc[indx[cond],'nrd_daystoevent'] - gdf.loc[anchor_1,'pseudoddate']\n",
    "    else:\n",
    "        anchor_1 = one_arr[0]\n",
    "        cond = (indx>anchor_1)\n",
    "        gdf.loc[indx[cond],'numdays_from_lastindex'] = gdf.loc[indx[cond],'nrd_daystoevent'] - gdf.loc[anchor_1,'pseudoddate']\n",
    "    return(gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = fsample.groupby(['nrd_visitlink'],group_keys=False).apply(compute_runningstats)\n",
    "s.drop('index', axis=1, inplace=True)\n",
    "fsample = s.copy()\n",
    "fsample['numdays_from_lastindex_na'] = 1\n",
    "fsample.loc[fsample['numdays_from_lastindex'] != -1, 'numdays_from_lastindex_na'] = 0\n",
    "del s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note to self: a better idea is to update make_onehot function to accomodate all types of columns for generating one-hot encoding \n",
    "def to_onehot(fsample, target_colname, labelspace_lst, prefix):\n",
    "    z = pd.DataFrame()\n",
    "    flag = fsample[target_colname].isnull().any()\n",
    "    fsample[\"$categ_col$\"] = fsample[target_colname]\n",
    "    fsample[\"$categ_col$\"] = fsample[\"$categ_col$\"].astype('category',categories=labelspace_lst)\n",
    "    onehot_df = pd.get_dummies(fsample[\"$categ_col$\"],prefix=prefix, dummy_na=flag)\n",
    "    z=pd.concat([z, onehot_df], axis=1)\n",
    "    del fsample[\"$categ_col$\"] # remove the temporary categorical variable\n",
    "    return(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_percent_missing(fsample,['zipinc_qrtl', 'pl_nchs', 'dmonth', 'dispuniform',\n",
    "                       'aweekend', 'los', 'dmonth', 'orproc','nchronic',\n",
    "                       'rehabtransfer','samedayevent', 'dispuniform',\n",
    "                       'mdc_nopoa', 'elective', 'rehabtransfer'])\n",
    "print()\n",
    "print()\n",
    "for elm in ('pay1','rehabtransfer', 'samedayevent', 'resident', 'mdc_nopoa'):\n",
    "    print(elm)\n",
    "    print(fsample[elm].value_counts())\n",
    "    check_percent_missing(fsample,[elm])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make one-hot encoding for income, location, discharge month, dispatch uniform\n",
    "cols = ['pay1', 'zipinc_qrtl', 'pl_nchs', 'dmonth', 'dispuniform', 'samedayevent', 'mdc_nopoa']\n",
    "prefixes = ['paysrc', 'income', 'loc', 'dmonth', 'dispuniform', 'samedayevent', 'mdcnopoa']\n",
    "for col, prefix in zip(cols, prefixes):\n",
    "    print(col)\n",
    "    label_lst = fsample[col].value_counts().index.tolist()\n",
    "    print(\"set of categories: \", label_lst)\n",
    "    onehot_df = to_onehot(fsample, col, label_lst, prefix)\n",
    "    fsample = pd.concat([fsample, onehot_df], axis=1)\n",
    "del cols, prefixes, label_lst, onehot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the features starting with a prefix\n",
    "def get_col_features(fsample, colprefix_lst):\n",
    "    fsamplecols = fsample.columns\n",
    "    fsamplecols_strseries = pd.Series(fsamplecols).str\n",
    "    colfeatures = []\n",
    "    for colprefix in colprefix_lst:\n",
    "        colfeatures += fsamplecols[fsamplecols_strseries.startswith(colprefix)].tolist()\n",
    "    return(colfeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load/process NRD severity database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nrd severity database\n",
    "nrd_severity = load_nrd('Severity')\n",
    "# snaity check\n",
    "print((nrd_severity.duplicated(subset=['key_nrd'])).sum())\n",
    "print(get_col_features(nrd_severity, ['cm']))\n",
    "\n",
    "check_percent_missing(nrd_severity, ['aprdrg_risk_mortality', 'aprdrg_severity'])\n",
    "for col in ['aprdrg_risk_mortality', 'aprdrg_severity']:\n",
    "    print(nrd_severity[col].value_counts())\n",
    "\n",
    "convert_dtypes(nrd_severity)\n",
    "# sanity check for dtype of key_nrd field in both data frames\n",
    "print(nrd_severity['key_nrd'].dtype == fsample['key_nrd'].dtype)\n",
    "\n",
    "fsample = pd.merge(fsample, nrd_severity, how='left', on='key_nrd') # join the two databases\n",
    "del nrd_severity # delete nrd_severity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding for aprdrg risk and mortality\n",
    "cols = ['aprdrg_risk_mortality', 'aprdrg_severity']\n",
    "prefixes = ['aprdrg_riskmortal', 'aprdrg_sever']\n",
    "for col, prefix in zip(cols, prefixes):\n",
    "    print(col)\n",
    "    label_lst = fsample[col].value_counts().index.tolist()\n",
    "    print(\"set of categories: \", label_lst)\n",
    "    onehot_df = to_onehot(fsample, col, label_lst, prefix)\n",
    "    fsample = pd.concat([fsample, onehot_df], axis=1)\n",
    "del cols, prefixes, label_lst, onehot_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load/process NRD dxpr database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load DX_PR_Grps database\n",
    "nrd_dxpr = load_nrd('Dxpr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_missing(nrd_dxpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build one-hot encoding for first chronic body systems indicator\n",
    "chronb_codes = set([])\n",
    "for i in range(1, 26):\n",
    "    chronb_codes = chronb_codes.union(set(nrd_dxpr['chronb{}'.format(i)].value_counts().index))\n",
    "chronb_lst = list(chronb_codes)\n",
    "onehot_df = make_onehot(nrd_dxpr, \"chronb\", chronb_lst, 1, \"chronbs\")\n",
    "nrd_dxpr = pd.concat([nrd_dxpr, onehot_df], axis=1)\n",
    "\n",
    "# generate new variables counting the chronic body system indicator codes occurrence\n",
    "for chronb_categ in chronb_lst:\n",
    "    cond = nrd_dxpr[[\"chronb{}\".format(i) for i in range(1, 26)]] == chronb_categ\n",
    "    print(cond.head())\n",
    "    nrd_dxpr['chronbscount_{}'.format(chronb_categ)] = cond.sum(axis=1)\n",
    "    print(\"-\"*40)\n",
    "del chronb_codes, chronb_lst, onehot_df, cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build one-hot encoding for procedure class\n",
    "pclass_codes = set([])\n",
    "for i in range(1, 16):\n",
    "    pclass_codes = pclass_codes.union(set(nrd_dxpr['pclass{}'.format(i)].value_counts().index))\n",
    "pclass_lst = list(pclass_codes)\n",
    "print(\"pclass_lst \", pclass_lst)\n",
    "for pclass_categ in pclass_lst:\n",
    "    cond = nrd_dxpr[[\"pclass{}\".format(i) for i in range(1, 16)]] == pclass_categ\n",
    "    print(cond.head())\n",
    "    nrd_dxpr['pclasscount_{}'.format(pclass_categ)] = cond.sum(axis=1)\n",
    "    print(\"-\"*40)\n",
    "del pclass_codes, pclass_lst, cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_missing(nrd_dxpr, get_col_features(nrd_dxpr, ['chronbs', 'pclasscount']))\n",
    "# reconvert data types\n",
    "convert_dtypes(nrd_dxpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity checks\n",
    "print((nrd_dxpr.duplicated(subset=['key_nrd'])).sum() == 0)\n",
    "print((nrd_dxpr.duplicated(subset=['key_nrd','hosp_nrd'])).sum() == 0)\n",
    "# cleaning and updating fsample joining cols to match with nrd_dxpr\n",
    "fsample['key_nrd'] = fsample['key_nrd'].astype('str')\n",
    "fsample.rename(index=str, columns={'hosp_nrd_x': 'hosp_nrd'}, inplace=True)\n",
    "fsample.drop(['hosp_nrd_y'], axis=1, inplace=True)\n",
    "# sanity checks\n",
    "print(fsample['hosp_nrd'].dtype == nrd_dxpr['hosp_nrd'].dtype)\n",
    "print(fsample['key_nrd'].dtype == nrd_dxpr['key_nrd'].dtype)\n",
    "fsample = pd.merge(fsample, nrd_dxpr, how='left', on=['key_nrd', 'hosp_nrd']) # join the two databases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump nrd_dxpr data frame on disk\n",
    "ReaderWriter.dump_data(nrd_dxpr, os.path.join(dataset_dir, 'nrd_dxpr.pkl'))\n",
    "del nrd_dxpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsample['time'] = fsample['run_num_events']\n",
    "cols = ['time', 'elective']\n",
    "prefixes = ['eventpos', 'elective']\n",
    "for col, prefix in zip(cols, prefixes):\n",
    "    print(\"column: \", col)\n",
    "    label_lst = fsample[col].value_counts().index.tolist()\n",
    "    print(\"categories: \", label_lst)\n",
    "    onehot_df = to_onehot(fsample, col, label_lst, prefix)\n",
    "    fsample = pd.concat([fsample, onehot_df], axis=1)\n",
    "del cols, prefixes, label_lst, onehot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_patientinfo_df(fsample):\n",
    "    target_cols = ['nrd_visitlink', 'female', 'age', 'allcause_readmit', 'run_num_indxevents', 'run_num_events']\n",
    "    # get the last event in every patient trajectory\n",
    "    pindx_df = fsample[target_cols].groupby('nrd_visitlink').nth(-1).reset_index()\n",
    "    groups = fsample.groupby(['nrd_visitlink'])\n",
    "    seqlen_df = groups.size().reset_index()\n",
    "    seqlen_df.columns = ['nrd_visitlink', 'seq_len']\n",
    "    pindx_df = pindx_df.merge(seqlen_df, left_on='nrd_visitlink', right_on='nrd_visitlink', how='left')\n",
    "    # count number of readmissions\n",
    "    count_allcausereadmit_df = groups['allcause_readmit'].sum().reset_index()\n",
    "    count_allcausereadmit_df.columns = ['nrd_visitlink', 'count_allcausereadmit']\n",
    "    pindx_df = pindx_df.merge(count_allcausereadmit_df, left_on='nrd_visitlink', right_on='nrd_visitlink', how='left')    \n",
    "    pindx_df['allcause_readmit_rate'] = pindx_df['count_allcausereadmit']/pindx_df['run_num_indxevents']\n",
    "    return(pindx_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint dump of dataset\n",
    "ReaderWriter.dump_data(fsample, os.path.join(dataset_dir, 'workingsample_HF_ckpt2.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep trajectories until the last HF event (i.e. last index event inclusive)\n",
    "s = keep_traj_uptoHF(fsample)\n",
    "# confirming that patients are preserved\n",
    "a = set(s['nrd_visitlink'].unique().tolist())\n",
    "b = set(fsample['nrd_visitlink'].unique().tolist())\n",
    "print(len(a) == len(b))\n",
    "# copy back s to sample_nrd\n",
    "fsample = s.copy()\n",
    "ReaderWriter.dump_data(fsample, os.path.join(dataset_dir, 'workingsample_HF_ckpt3_trunctraj.pkl'))\n",
    "del s, a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate previous readmission status\n",
    "def shift_allcausereadmit(gdf):\n",
    "    gdf['admit_state'] = gdf['allcause_readmit'].shift(1)\n",
    "    return(gdf)\n",
    "\n",
    "traj_info = prepare_patientinfo_df(fsample)\n",
    "seqlen_df = traj_info[['nrd_visitlink', 'seq_len']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 123\n",
    "# we extract stratified 5-fold cross validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# stratified_cv = StratifiedKFold(traj_info['allcause_readmit'], n_folds=5, random_state=RANDOM_STATE) # for reproducibility\n",
    "stratified_cv = StratifiedKFold(n_splits=5, random_state=RANDOM_STATE)# for reproducibility\n",
    "fold_c = 0\n",
    "ref_numinstances = traj_info.shape[0]\n",
    "datasplit = []\n",
    "for train_index, test_index in stratified_cv.split(traj_info['allcause_readmit'],traj_info['allcause_readmit']):\n",
    "    print(\"fold_c \", fold_c)\n",
    "    print(\"train_index.shape \", train_index.shape)\n",
    "    print(\"test_index.shape \", test_index.shape)\n",
    "    num_instances = train_index.shape[0]+test_index.shape[0]\n",
    "    print(\"sum of train and test \", num_instances)\n",
    "    # check if the cross validation division result to total instances number\n",
    "    assert num_instances == ref_numinstances\n",
    "    # generate training fold data\n",
    "    train_fold = traj_info.iloc[train_index]['nrd_visitlink']\n",
    "    test_fold = traj_info.iloc[test_index]['nrd_visitlink']\n",
    "    datasplit.append((train_fold, test_fold))\n",
    "    fold_c+=1\n",
    "# sanity check\n",
    "for elm in datasplit:\n",
    "    train, test = elm\n",
    "    print(\"intersection between train and test set: \", set(train).intersection(set(test)))\n",
    "    print(\"train size:{}, test size:{}\".format(len(train), len(test)))\n",
    "# dump datasplit and traj_info on disk\n",
    "ReaderWriter.dump_data(datasplit, os.path.join(dataset_dir, 'datasplit.pkl'))\n",
    "ReaderWriter.dump_data(traj_info, os.path.join(dataset_dir, 'traj_info.pkl'))\n",
    "del fold_c, ref_numinstances, num_instances, train_fold, test_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compute average sequence length and allcause readmission rate in the split samples\n",
    "# ids_type=['train', 'test']\n",
    "# for fnum, tup in enumerate(datasplit):\n",
    "#     print(\"fold \", fnum)\n",
    "#     for i, ids in enumerate(tup):\n",
    "#         cond = traj_info['nrd_visitlink'].isin(ids)\n",
    "#         print('{} stats:'.format(ids_type[i]))\n",
    "#         print(\"average sequence length: \", traj_info.loc[cond, 'seq_len'].sum()/len(ids))\n",
    "#         print(\"average readmission rate: \", traj_info.loc[cond, 'allcause_readmit_rate'].sum()/len(ids))\n",
    "#         print(\"last event readmission rate:\", traj_info.loc[cond, 'allcause_readmit'].sum()/len(ids))\n",
    "#         print(\"-\"*30)\n",
    "#     print(\"*\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_countcols(top_df, prefix, threshold):\n",
    "    target_vars = top_df.loc[top_df['count'] > threshold, 'ccs_code']\n",
    "    processed_vars = []\n",
    "    return([\"{}_{}\".format(prefix, float(elm)) for elm in target_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold for the count of categories of diagnosis/procedures to keep\n",
    "count_threshold = 1000\n",
    "diag_targetcols = []\n",
    "for prefix in (1,2,3,'count'):\n",
    "    diag_targetcols +=get_countcols(top_diagn_df, 'diagn{}'.format(prefix), count_threshold)\n",
    "    if(type(prefix) == int):\n",
    "        diag_targetcols += ['diagn{}_nan'.format(prefix)] \n",
    "proc_targetcols = []\n",
    "for prefix in (1,2,3,'count'):\n",
    "    proc_targetcols +=get_countcols(top_proc_df, 'proc{}'.format(prefix), count_threshold)\n",
    "    if(type(prefix) == int):\n",
    "        proc_targetcols += ['proc{}_nan'.format(prefix)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process/compute previous admission state\n",
    "fsample=fsample.groupby('nrd_visitlink').apply(shift_allcausereadmit)\n",
    "fsample.loc[fsample['admit_state'].isnull(), 'admit_state']=2\n",
    "fsample['deltat_na'] = 0 # case of delta_t is not applicable (i.e. first event where we do not have info on previous events)\n",
    "fsample.loc[fsample['admit_state']==2,'deltat_na']=1\n",
    "fsample['admit_state']= fsample['admit_state'].astype('int')\n",
    "# one-hot encoding \n",
    "# Note to self: collect all variables for one-hot enconding in one place instead of sporadic computation of each independently\n",
    "cols = ['admit_state']\n",
    "prefixes = ['admitstate']\n",
    "for col, prefix in zip(cols, prefixes):\n",
    "    print(\"col: \", col)\n",
    "    label_lst = fsample[col].value_counts().index.tolist()\n",
    "    print(\"categories: \", label_lst)\n",
    "    onehot_df = to_onehot(fsample, col, label_lst, prefix)\n",
    "    fsample = pd.concat([fsample, onehot_df], axis=1)\n",
    "del cols, prefixes, label_lst, onehot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all targeted features/variables\n",
    "COL_FEATURES = diag_targetcols + proc_targetcols + \\\n",
    "               get_col_features(fsample, ['ecode1_', 'ecodecount', \n",
    "                                          'chronbs', 'pclasscount', \n",
    "                                          'aprdrg_riskmortal', 'aprdrg_sever_',\n",
    "                                          'mdcnopoa']) + \\\n",
    "               get_col_features(fsample, ['income', 'loc', 'dmonth_', 'dispuniform_', 'paysrc',\n",
    "                                          'samedayevent_']) + \\\n",
    "               get_col_features(fsample, ['female', 'aweekend', 'cm', 'elective_', \n",
    "                                          'resident', 'rehabtransfer', 'orproc', 'deltat_na']) + \\\n",
    "               get_col_features(fsample, ['age', 'los', 'deltat_prevevent', 'nchronic', \n",
    "                                          'run_num_indxevents', 'run_num_events'])   \n",
    "\n",
    "# all cateogrical features\n",
    "cat_cols = ['dxccs1','dxccs2','dxccs3','prccs1','prccs2','prccs3','chronb1', 'e_ccs1', 'aprdrg_risk_mortality', \n",
    "            'aprdrg_severity', 'mdc_nopoa', 'pay1', 'zipinc_qrtl', 'pl_nchs', 'dmonth', 'dispuniform', \n",
    "            'samedayevent', 'female', 'aweekend',\n",
    "            'rehabtransfer', 'resident', 'orproc', 'elective', 'deltat_na', \n",
    "            'cm_aids', 'cm_alcohol', 'cm_anemdef', 'cm_arth', 'cm_bldloss', 'cm_chf', 'cm_chrnlung', 'cm_coag', \n",
    "            'cm_depress', 'cm_dm', 'cm_dmcx', 'cm_drug', 'cm_htn_c', 'cm_hypothy', 'cm_liver', 'cm_lymph', 'cm_lytes',\n",
    "            'cm_mets', 'cm_neuro', 'cm_obese', 'cm_para', 'cm_perivasc', 'cm_psych', 'cm_pulmcirc', 'cm_renlfail', \n",
    "            'cm_tumor', 'cm_ulcer', 'cm_valve', 'cm_wghtloss']\n",
    "# all continuous features\n",
    "CONT_COLS = get_countcols(top_diagn_df, 'diagncount'.format(prefix), count_threshold) + \\\n",
    "            get_countcols(top_proc_df, 'proccount'.format(prefix), count_threshold) + \\\n",
    "            get_col_features(fsample, ['ecodecount','chronbscount', 'pclasscount', \n",
    "                                       'age', 'los','deltat_prevevent','nchronic',\n",
    "                                       'run_num_indxevents','run_num_events'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize all continuous features\n",
    "for colprefix, normalizer in (('diagncount', 25), ('proccount', 15), ('ecodecount', 4), ('chronbscount', 25),\n",
    "                              ('pclasscount', 15), ('age', 90), ('los', 365), \n",
    "                              ('deltat_prevevent', 364), ('nchronic', 25),\n",
    "                              ('run_num_indxevents', 180), ('run_num_events', 180)):\n",
    "    target_cols = get_col_features(fsample, [colprefix])\n",
    "    fsample[target_cols] = fsample[target_cols]/normalizer\n",
    "# sanity check -- normalization\n",
    "for col in CONT_COLS:\n",
    "    print(\"colname: \", col)\n",
    "    print(fsample[col].describe())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read additional variables description\n",
    "compl_targetvar_df = pd.read_excel(os.path.join(project_dir,'variables', 'complementary_targetvars.xlsx'))\n",
    "# prepare variable name-> description mapping\n",
    "# prepare variable code-> name mapping\n",
    "feat_label = {}\n",
    "code_feat = {}\n",
    "rank_map = {1:'Primary',\n",
    "            2:'Secondary',\n",
    "            3:'Tertiary'}\n",
    "prefix_map = {'d':'diagnosis',\n",
    "              'p':'procedure'}\n",
    "for colnames, info_df in ((diag_targetcols, top_diagn_df), \n",
    "                          (proc_targetcols, top_proc_df)):\n",
    "    for colname in colnames:\n",
    "        code_feat[len(feat_label)] = colname\n",
    "        prefix, code = colname.split(\"_\")\n",
    "        if(code == 'nan'):\n",
    "            conv_code = 'Missing'\n",
    "        else:\n",
    "            conv_code = str(int(float(code)))\n",
    "            conv_code = info_df.loc[info_df['ccs_code']==conv_code, 'ccs_label'].tolist()[0]\n",
    "        if('count' not in prefix):\n",
    "            rank = int(prefix[-1])\n",
    "            feat_label[colname] = '[{} {}] '.format(rank_map[rank], prefix_map[prefix[0]]) + conv_code\n",
    "        else:\n",
    "            feat_label[colname] = '[Count {}] '.format(prefix_map[prefix[0]]) + conv_code\n",
    "\n",
    "# (varname in fsample, varname in info_df, vartype, string to prefix the description)\n",
    "for varname_a, varname_b, vartype, addendum in (('ecode1_', 'ecode', 'N','[Primary]'), \n",
    "                                                ('ecodecount', 'ecode', 'N', '[Count]'),\n",
    "                                                ('chronbs1_', 'chronb', 'N', '[Primary]'), \n",
    "                                                ('chronbscount', 'chronb', 'N', '[Count]'),\n",
    "                                                ('pclasscount', 'pclass', 'N', '[Count]'),\n",
    "                                                ('aprdrg_riskmortal_', 'aprdrg_risk_mortality', 'N', ''),\n",
    "                                                ('aprdrg_sever_', 'aprdrg_severity', 'N', ''),\n",
    "                                                ('mdcnopoa', 'mdcnopoa', 'N', ''),\n",
    "                                                ('income', 'zipinc_qrtl', 'N', ''), \n",
    "                                                ('loc', 'pl_nchs', 'N', ''),\n",
    "                                                ('dmonth_','dmonth', 'N', ''),\n",
    "                                                ('dispuniform_','dispuniform', 'N', ''),\n",
    "                                                ('paysrc', 'pay1', 'N', ''),\n",
    "                                                ('samedayevent_', 'samedayevent', 'N', ''),\n",
    "                                                ('female', 'female', 'B', ''), \n",
    "                                                ('aweekend', 'aweekend', 'B', ''),\n",
    "                                                ('cm', 'cm', 'H', 'comorbidity:'), \n",
    "                                                ('elective_', 'elective', 'N',''), \n",
    "                                                ('resident', 'resident', 'B',''), \n",
    "                                                ('rehabtransfer', 'rehabtransfer', 'B',''),\n",
    "                                                ('orproc', 'orproc', 'B',''),\n",
    "                                                ('deltat_na', 'deltat_na', 'B', '')):\n",
    "    if(vartype != 'H'):\n",
    "        info_df = compl_targetvar_df.loc[compl_targetvar_df['colname']==varname_b].copy()\n",
    "    for colname in get_col_features(fsample, [varname_a]):\n",
    "        print(\"colname: \", colname)\n",
    "        if(vartype == 'N'):\n",
    "            if('_' in varname_a):\n",
    "                split_str = varname_a\n",
    "            else:\n",
    "                split_str = varname_a + '_'\n",
    "            code = colname.split(split_str)[-1]\n",
    "            if(code == 'nan'):\n",
    "                conv_code = varname_a + ' Missing'\n",
    "            else:\n",
    "                conv_code = int(float(code))\n",
    "                conv_code = info_df.loc[info_df['value_code']==conv_code, 'label'].tolist()[0]\n",
    "\n",
    "        elif(vartype == 'B'):\n",
    "            conv_code = info_df['label'].tolist()[0]\n",
    "        elif(vartype == 'H'):\n",
    "            conv_code = compl_targetvar_df.loc[compl_targetvar_df['colname']==colname, 'label'].tolist()[0]\n",
    "        if(addendum):\n",
    "            conv_code = addendum + ' ' + conv_code\n",
    "        code_feat[len(feat_label)] = colname\n",
    "        feat_label[colname] = conv_code\n",
    "        \n",
    "for varname, desc in (('age', 'Age'),\n",
    "                      ('los', 'length of stay'),\n",
    "                      ('deltat_prevevent', 'Number of days from last any hospitalization discharge'),\n",
    "                      ('nchronic', 'number of registered chronic conditions'),\n",
    "                      ('run_num_indxevents', 'Number of index events in the timeline up to current admission (inclusive)'),\n",
    "                      ('run_num_events', 'Number of hospitalization events in the timeline up to current admission (inclusive)')\n",
    "                     ):\n",
    "    code_feat[len(feat_label)]=varname\n",
    "    feat_label[varname] = desc\n",
    "# sanity checks\n",
    "print(len(code_feat) == len(feat_label))\n",
    "print(set(COL_FEATURES) - set(code_feat.values()))\n",
    "assert len(code_feat) == len(COL_FEATURES)\n",
    "assert len(feat_label) == len(code_feat)\n",
    "code_feat_inv = {colname:code for code, colname in code_feat.items()}\n",
    "del rank_map, prefix_map, prefix, code, conv_code, rank, info_df, split_str, code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a map for target variables to be used in feature contribution analysis\n",
    "switch_onoff = {}\n",
    "prefixes = [('diagncount_', 3, 'diagn'), \n",
    "            ('proccount_', 3, 'proc'), \n",
    "            ('ecodecount_', 1, 'ecode'),\n",
    "            ('chronbscount_',1, 'chronbs'),\n",
    "            ('pclasscount_', 1, None)]\n",
    "indices_set = set()\n",
    "for tup_elm in prefixes:\n",
    "    a, b, c = tup_elm\n",
    "    print(a, b, c)\n",
    "    for colname in get_col_features(fsample, [a]):\n",
    "        if(colname in code_feat_inv):\n",
    "            print(\"colname: \", colname)\n",
    "            code_num = colname.split(a)[-1]\n",
    "            print(\"codenum: \", code_num)\n",
    "            index_key = code_feat_inv[colname]\n",
    "            switch_onoff[index_key] = [index_key]\n",
    "            if(c):\n",
    "                switch_onoff[index_key] += [code_feat_inv[\"{}{}_{}\".format(c, rank, code_num)] for rank in range(1, b+1)]\n",
    "            indices_set.update(set(switch_onoff[index_key]))\n",
    "# deltat_prevevent\n",
    "index_key = code_feat_inv['deltat_prevevent']\n",
    "switch_onoff[index_key] = [index_key, code_feat_inv['deltat_na']]\n",
    "indices_set.update(set(switch_onoff[index_key]))\n",
    "\n",
    "remain_indices = set(code_feat.keys()) - indices_set\n",
    "\n",
    "for indx in remain_indices:\n",
    "    if(code_feat[indx].split(\"_\")[-1] not in {'na', 'nan'}):\n",
    "        switch_onoff[indx] = [indx]\n",
    "# print the switch_onoff map\n",
    "for indexkey, target_lst in switch_onoff.items():\n",
    "    print(\"switch on/off: \", code_feat[indexkey])\n",
    "    print(\"name: \", feat_label[code_feat[indexkey]])\n",
    "    for elm in target_lst:\n",
    "        print(\"target indices: \", code_feat[elm])\n",
    "    print(\"-\"*35)\n",
    "ReaderWriter.dump_data(switch_onoff, os.path.join(dataset_dir, 'switch_onoff_features.pkl'))\n",
    "del prefixes, indices_set, a, b, c, code_num, index_key, remain_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsample['rindx'] = fsample.index.tolist()\n",
    "# create a variable to indicate to last event\n",
    "fsample['lastevent'] = 0\n",
    "lastevent_indx = fsample.groupby('nrd_visitlink').nth(-1)['rindx'].tolist()\n",
    "fsample.loc[fsample['rindx'].isin(lastevent_indx), 'lastevent']=1\n",
    "del lastevent_indx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump data on disk\n",
    "ReaderWriter.dump_data(CONT_COLS, os.path.join(dataset_dir, 'continuous_features.pkl'))\n",
    "ReaderWriter.dump_data(COL_FEATURES, os.path.join(dataset_dir, 'col_features.pkl'))\n",
    "ReaderWriter.dump_data(feat_label, os.path.join(dataset_dir, 'feat_label.pkl'))\n",
    "ReaderWriter.dump_data(code_feat, os.path.join(dataset_dir, 'code_feat.pkl'))\n",
    "ReaderWriter.dump_data(fsample, os.path.join(dataset_dir, 'fsample.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General dataset summary and some plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_diagproc(top_df, K, fname):\n",
    "    tmp = top_diagn_df.iloc[:20]\n",
    "    ax = tmp['count_percent'].plot.barh(figsize=(16,9));\n",
    "    labels =tmp['ccs_label']\n",
    "    ax.set_yticklabels(labels, fontsize=12)\n",
    "    ax.set_xlabel('% percentage')\n",
    "    ax.get_figure().savefig(fname, format='svg')\n",
    "#     plt.axvline(0, color='black')\n",
    "#     ax.set_ylabel(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_diagn_df['count_percent'] = 100*top_diagn_df['count']/top_diagn_df['count'].sum()\n",
    "K = 25\n",
    "plot_top_diagproc(top_diagn_df, K, os.path.join(dataset_dir, 'top_{}_diagnosis.svg'.format(K)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_proc_df['count_percent'] = 100*top_proc_df['count']/top_proc_df['count'].sum()\n",
    "plot_top_diagproc(top_proc_df, K, os.path.join(dataset_dir, 'top_{}_procedures.svg'.format(K)))\n",
    "del K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot paysrc barplot percentage\n",
    "ax = fsample['pay1'].value_counts(normalize=True).plot.barh(figsize=(16,9));\n",
    "labels=[]\n",
    "for i in fsample['pay1'].value_counts().index.tolist():\n",
    "    key = 'paysrc_{}'.format(float(i))\n",
    "    labels.append(feat_label[key])\n",
    "ax.set_yticklabels(labels, fontsize=12)\n",
    "ax.set_xlabel('% percentage')\n",
    "ax.get_figure().savefig(os.path.join(dataset_dir, 'payment_category.svg'),format='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"total number of patients: \\n\", fsample['nrd_visitlink'].unique().shape)\n",
    "print()\n",
    "print(\"age:\\n\", fsample_levent['age'].describe())\n",
    "print()\n",
    "print(\"gender:\\n\", fsample_levent['female'].value_counts(normalize=True))\n",
    "print(\"gender:\\n\", fsample_levent['female'].value_counts())\n",
    "print()\n",
    "print(\"number of index events:\\n\", fsample['index_event'].value_counts())\n",
    "print(\"number of index events:\\n\", fsample['index_event'].value_counts(normalize=True))\n",
    "print()\n",
    "print(\"timeline length: \", fsample_levent['time'].describe())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patients timeline % barplot\n",
    "ax = fsample_levent['time'].value_counts(normalize=True).plot.barh(figsize=(16,9));\n",
    "ax.set_xlabel('% percentage')\n",
    "ax.set_ylabel('Timeline length (number of events)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
