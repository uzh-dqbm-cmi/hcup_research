{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we define relevant directories\n",
    "import sys\n",
    "import os\n",
    "import copy\n",
    "# project directory\n",
    "project_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "# src directory \n",
    "src_dir = os.path.join(project_dir, 'src')\n",
    "sys.path.insert(0, src_dir)\n",
    "print(\"src_dir \", src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from train_eval import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_PROCESSORS = mp.cpu_count() - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the available devices\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '' # specify the visibile devices before running the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_models(bestmodels_dir, wrk_dir, model_class, y_codebook, num_epochs, \n",
    "                      to_gpu, memmap, dataset_dir, func_mode = 'train', options={}):\n",
    "    \n",
    "    if(func_mode == 'train'):\n",
    "        target_folder = 'best_model'\n",
    "        dsettypes = ['train', 'validation']\n",
    "    elif(func_mode == 'validation'):\n",
    "        target_folder = None\n",
    "        dsettypes = ['validation']\n",
    "    else:\n",
    "        target_folder = None\n",
    "        dsettypes = ['test']\n",
    "    \n",
    "    bestmodels_lst = retrieve_bestmodels(bestmodels_dir, target_folder)\n",
    "    folds_dir = [\"fold_{}_none\".format(i) for i in range(5)]\n",
    "            \n",
    "    classweight_option = options.get('classweight_option')\n",
    "    if(model_class == 'RNN_Labeler'):\n",
    "        model_train = rnn_run\n",
    "    elif(model_class ==  'RNNSS_Labeler'):\n",
    "        model_train = rnnss_run\n",
    "    # to enforce what options are available    \n",
    "    elif(model_class in {'CRF_Labeler', 'CRF_Pair_Labeler', 'RNNCRF_Pair_Labeler', 'RNNCRF_Unary_Labeler'}): \n",
    "        stopstate_symb = options.get('stopstate_symb')\n",
    "        if(stopstate_symb):\n",
    "            y_codebook[stopstate_symb] = len(y_codebook)\n",
    "        model_train = rnncrf_run\n",
    "    elif(model_class in {'NN_Labeler', 'CNN_Labeler', 'CNNWide_Labeler'}):\n",
    "        model_train = cnn_nn_run\n",
    "        \n",
    "    # prepare the arguments for function call\n",
    "    rank = 0\n",
    "    args = []\n",
    "    evaldirs = []\n",
    "    for bestmodel in bestmodels_lst:\n",
    "        bestmodel_config = ReaderWriter.read_data(bestmodel.model_config_pth)\n",
    "        model_statedict_pth = bestmodel.model_statedict_pth\n",
    "        model_prefix = bestmodel.model_prefix\n",
    "        for fold in folds_dir:\n",
    "            print(\"current_fold: \", fold)\n",
    "            if(func_mode == 'test'):\n",
    "                # case of the fold not matching\n",
    "                if(len(model_prefix.split(fold)) == 1):\n",
    "                    continue\n",
    "                else:\n",
    "                    mweights_path = model_statedict_pth\n",
    "            elif(func_mode == 'train'):\n",
    "                mweights_path = None\n",
    "            elif(func_mode == 'validation'):\n",
    "                mweights_path = model_statedict_pth\n",
    "\n",
    "            # make a copy of options\n",
    "            options_c = copy.deepcopy(options)\n",
    "            target_dsets, input_dim = load_dataset(os.path.join(dataset_dir, fold, 'dataset_tuple.pkl'), dsettypes, memmap)\n",
    "            if(classweight_option):\n",
    "                class_weights = ReaderWriter.read_data(os.path.join(dataset_dir, \n",
    "                                                                    fold, \n",
    "                                                                    'train_pdtm', \n",
    "                                                                    'classweights_'+classweight_option+'.pkl'))\n",
    "                options_c['class_weights'] = class_weights        \n",
    "            eval_dir = create_directory('{}_{}'.format(bestmodel.model_prefix, fold), wrk_dir)\n",
    "            evaldirs.append(eval_dir)\n",
    "            # add the fold_id option\n",
    "            options_c['fold_id'] = fold\n",
    "            args.append((bestmodel_config, model_class, target_dsets, y_codebook, \n",
    "                         num_epochs, eval_dir, rank, to_gpu, memmap, model_statedict_pth, options_c))\n",
    "\n",
    "            rank +=1\n",
    "    \n",
    "    # multiprocessing the model runs\n",
    "    outlog = os.path.join(wrk_dir, 'out.log')\n",
    "\n",
    "    num_models = len(args)\n",
    "    num_processors = mp.cpu_count()-1\n",
    "\n",
    "    if(num_processors > NUM_PROCESSORS):\n",
    "        num_processors = NUM_PROCESSORS\n",
    "    if(num_models < num_processors):\n",
    "        num_processors = num_models\n",
    "    print(\"Number of models to be evaluated: \", num_models)\n",
    "\n",
    "    processes = []\n",
    "    counter = 0\n",
    "    tick = datetime.datetime.now()\n",
    "    while len(args):\n",
    "        if(len(args) < num_processors):\n",
    "            num_processors = len(args)\n",
    "        for __ in range(num_processors):\n",
    "            arg = args.pop()\n",
    "            p = mp.Process(target=model_train, args=arg)\n",
    "            p.start()\n",
    "            processes.append(p)\n",
    "            print(\"counter \", counter)\n",
    "            counter+=1\n",
    "        for p in processes:\n",
    "            p.join()\n",
    "            print(\"we are joining {}\".format(p))\n",
    "            \n",
    "    tock = datetime.datetime.now()\n",
    "    line = \"total number of seconds: {}\\n\".format((tock-tick).seconds)\n",
    "    ReaderWriter.write_log(line, outlog, mode='a')\n",
    "    if(func_mode == 'train'):\n",
    "        dtype = 'validation'\n",
    "    elif(func_mode == 'test'):\n",
    "        dtype = 'test'\n",
    "    elif(func_mode == 'validation'):\n",
    "        dtype = 'test'\n",
    "    scores = get_perfscores(evaldirs, dtype, outlog, None)\n",
    "    return(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_dir = create_directory('decoded_output', project_dir)\n",
    "hyperparam_optimzmodel_dir =  create_directory('hyperparam_optimz_models', project_dir)\n",
    "dataset_dir = os.path.join(project_dir, 'dataset_neural')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate each model run/test in a cell -- this can be easily optmized to run all models at once!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN/RNNSS train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# checking __main__ condition helps preventing \"weird\" issues when running multiprocesses on Windows OS!!\n",
    "if __name__ == \"__main__\":\n",
    "    to_gpu = True # run on GPU\n",
    "    memmap = True # data is memory mapped originally using numpy memmap\n",
    "    bidirection = False\n",
    "    classweight_option = 'last_indx'\n",
    "    loss_modes = ('Convex_HF_LastHF', 'LastHF', 'Uniform_HF', 'Convex_HF_NonHF')\n",
    "    rgrad_mode = 'clip_norm'\n",
    "    rgrad_limit = (None, 1)\n",
    "    model_classes = ('RNN_Labeler', 'RNNSS_Labeler')\n",
    "    prefixes = ('rnn', 'rnnss')\n",
    "    y_codebooks = ({0:0, 1:1}, {0:0, 1:1, '__START__':2})\n",
    "    \n",
    "    # train/validation phase\n",
    "    num_epochs = 2\n",
    "    for i, model_class in enumerate(model_classes):\n",
    "        y_codebook = y_codebooks[i]\n",
    "        prefix = prefixes[i]\n",
    "        # where to find the model with *best* hyperparams      \n",
    "        target_dir = create_directory(prefix, hyperparam_optimzmodel_dir)\n",
    "        trainmodel_dir = os.path.join(project_dir, 'train_models', prefix)\n",
    "        for loss_mode in loss_modes:\n",
    "            fname = \"{}_{}\".format(loss_mode, rgrad_mode+str(rgrad_limit[-1]))\n",
    "            wrk_dir = create_directory('{}_{}_train'.format(prefix, fname), trainmodel_dir)\n",
    "            scoredict = train_eval_models(target_dir, wrk_dir, model_class, y_codebook, num_epochs,\n",
    "                                          to_gpu, memmap, dataset_dir,'train',\n",
    "                                         {'classweight_option':classweight_option,\n",
    "                                          'reg_type':'l2',\n",
    "                                          'loss_mode':loss_mode,\n",
    "                                          'bidirection':bidirection,\n",
    "                                          'rgrad_mode':rgrad_mode,\n",
    "                                          'rgrad_limit':rgrad_limit})\n",
    "            print(\"training:\")\n",
    "            print(scoredict)\n",
    "            print(\"-\"*25)\n",
    "            \n",
    "    # test phase\n",
    "    num_epochs = 1\n",
    "    for i, model_class in enumerate(model_classes):\n",
    "        y_codebook = y_codebooks[i]\n",
    "        prefix = prefixes[i]\n",
    "        trainmodel_dir = os.path.join(project_dir, 'train_models', prefix)\n",
    "        testmodel_dir = os.path.join(project_dir, 'test_models', prefix)\n",
    "        for loss_mode in loss_modes:\n",
    "            fname = \"{}_{}\".format(loss_mode, rgrad_mode+str(rgrad_limit[-1]))\n",
    "            target_dir = create_directory('{}_{}_train'.format(prefix, fname), trainmodel_dir)\n",
    "            wrk_dir = create_directory('{}_{}_test'.format(prefix, fname), testmodel_dir)\n",
    "            scoredict = train_eval_models(target_dir, wrk_dir, model_class, y_codebook, num_epochs,\n",
    "                                          to_gpu, memmap, dataset_dir,'test',\n",
    "                                         {'classweight_option':classweight_option,\n",
    "                                          'reg_type':'l2',\n",
    "                                          'loss_mode':loss_mode,\n",
    "                                          'bidirection':bidirection,\n",
    "                                          'dec_outdir':create_directory('{}_lossmode_{}_decoded'.format(prefix, loss_mode), decoded_dir)})\n",
    "            print(\"testing:\")\n",
    "            print(scoredict)\n",
    "            print(\"-\"*25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN/CNNWide/MLP train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":    \n",
    "    to_gpu = True\n",
    "    memmap = True\n",
    "    classweight_option = 'last_indx'\n",
    "    y_codebook = {0:0,1:1}\n",
    "    rgrad_mode = 'clip_norm'\n",
    "    rgrad_limit = (None, 1)\n",
    "    model_classes = ('CNN_Labeler', 'CNNWide_Labeler', 'NN_Labeler')\n",
    "    prefixes = ('cnn', 'cnnwide', 'nn')\n",
    "    # train/validation phase\n",
    "    num_epochs = 2\n",
    "    for i, model_class in enumerate(model_classes):\n",
    "        prefix = prefixes[i]\n",
    "        target_dir = create_directory(prefix, hyperparam_optimzmodel_dir)\n",
    "        trainmodel_dir = os.path.join(project_dir, 'train_models', prefix)\n",
    "        wrk_dir = create_directory('{}_train'.format(prefix), trainmodel_dir)\n",
    "        scoredict = train_eval_models(target_dir, wrk_dir, model_class, y_codebook, num_epochs,\n",
    "                                      to_gpu, memmap, dataset_dir,'train',\n",
    "                                     {'classweight_option':classweight_option,\n",
    "                                      'reg_type':'l2',\n",
    "                                      'rgrad_mode':rgrad_mode,\n",
    "                                      'rgrad_limit':rgrad_limit})\n",
    "        print(\"training:\")\n",
    "        print(scoredict)\n",
    "        print(\"-\"*25)\n",
    "    # test phase\n",
    "    num_epochs = 1\n",
    "    for i, model_class in enumerate(model_classes):\n",
    "        prefix = prefixes[i]\n",
    "        trainmodel_dir = os.path.join(project_dir, 'train_models', prefix)\n",
    "        target_dir = create_directory('{}_train'.format(prefix), trainmodel_dir)\n",
    "        testmodel_dir = os.path.join(project_dir, 'test_models', prefix)\n",
    "        wrk_dir = create_directory('{}_test'.format(prefix), testmodel_dir)\n",
    "        scoredict = train_eval_models(target_dir, wrk_dir, model_class, y_codebook, num_epochs,\n",
    "                                      to_gpu, memmap, dataset_dir, 'test',\n",
    "                                      {'classweight_option':classweight_option,\n",
    "                                       'dec_outdir':create_directory('{}_decoded'.format(prefix), decoded_dir)}) \n",
    "        print(\"testing:\")\n",
    "        print(scoredict)\n",
    "        print(\"-\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNCRF {Pair, Unary} train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    to_gpu = True\n",
    "    memmap = True\n",
    "    bidirection = False\n",
    "    y_codebook = {0:0, 1:1, '__START__':2}\n",
    "    decoder_type = 'viterbi'\n",
    "    rgrad_mode = 'clip_norm'\n",
    "    rgrad_limit = (None, 1)\n",
    "    model_classes = ('RNNCRF_Pair_Labeler', 'RNNCRF_Unary_Labeler')\n",
    "    prefixes = ('rnncrfpair', 'rnncrfunary')\n",
    "    # train/validation phase\n",
    "    num_epochs = 2\n",
    "    for i, model_class in enumerate(model_classes):\n",
    "        prefix = prefixes[i]\n",
    "        target_dir = create_directory(prefix, hyperparam_optimzmodel_dir)\n",
    "        trainmodel_dir = os.path.join(project_dir, 'train_models', prefix)\n",
    "        wrk_dir = create_directory('{}_train'.format(prefix), trainmodel_dir)\n",
    "        scoredict = train_eval_models(target_dir, wrk_dir, model_class, y_codebook, num_epochs,\n",
    "                                      to_gpu, memmap, dataset_dir,'train',\n",
    "                                     {'reg_type':'l2',\n",
    "                                      'decoder_type':decoder_type,\n",
    "                                      'bidirection':bidirection,\n",
    "                                      'rgrad_mode':rgrad_mode,\n",
    "                                      'rgrad_limit':rgrad_limit})\n",
    "        print(\"training:\")\n",
    "        print(scoredict)\n",
    "        print(\"-\"*25)\n",
    "    # test phase\n",
    "    num_epochs = 1\n",
    "    for i, model_class in enumerate(model_classes):\n",
    "        prefix = prefixes[i]\n",
    "        trainmodel_dir = os.path.join(project_dir, 'train_models', prefix)\n",
    "        target_dir = create_directory('{}_train'.format(prefix), trainmodel_dir)\n",
    "        testmodel_dir = os.path.join(project_dir, 'test_models', prefix)\n",
    "        wrk_dir = create_directory('{}_test'.format(prefix), testmodel_dir)\n",
    "        scoredict = train_eval_models(target_dir, wrk_dir, model_class, y_codebook, num_epochs,\n",
    "                                      to_gpu, memmap, dataset_dir,'test',\n",
    "                                     {'reg_type':'l2',\n",
    "                                      'decoder_type':decoder_type,\n",
    "                                      'bidirection':bidirection,\n",
    "                                      'dec_outdir':create_directory('{}_decoded'.format(prefix), decoded_dir)})\n",
    "        print(\"testing:\")\n",
    "        print(scoredict)\n",
    "        print(\"-\"*25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRF/Neural CRF {Pair, Unary} train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    to_gpu = True\n",
    "    memmap = True\n",
    "    y_codebook = {0:0, 1:1, '__START__':2}\n",
    "    decoder_type = 'viterbi'\n",
    "    rgrad_mode = 'clip_norm'\n",
    "    rgrad_limit = (None, 1)\n",
    "    model_classes = ('CRF_Pair_Labeler', 'CRF_Labeler')\n",
    "    crf_types = ('nn','only')\n",
    "    # train/validation phase\n",
    "    num_epochs = 2\n",
    "    for model_class in model_classes:\n",
    "        for crf_type in crf_types:\n",
    "            if('Pair' in model_class):\n",
    "                prefix = 'crf{}pair'.format(crf_type)\n",
    "            else:\n",
    "                prefix = 'crf{}'.format(crf_type)\n",
    "            target_dir = create_directory(prefix, hyperparam_optimzmodel_dir)\n",
    "            trainmodel_dir = os.path.join(project_dir, 'train_models', prefix)\n",
    "            wrk_dir = create_directory('{}_train'.format(prefix), trainmodel_dir)\n",
    "            scoredict = train_eval_models(target_dir, wrk_dir, model_class, y_codebook, num_epochs,\n",
    "                                          to_gpu, memmap, dataset_dir,'train',\n",
    "                                         {'reg_type':'l2',\n",
    "                                          'decoder_type':decoder_type,\n",
    "                                          'crf_type': '{}_crf'.format(crf_type),\n",
    "                                          'bidirection':bidirection,\n",
    "                                          'rgrad_mode':rgrad_mode,\n",
    "                                          'rgrad_limit':rgrad_limit})\n",
    "            print(\"training:\")\n",
    "            print(scoredict)\n",
    "            print(\"-\"*25)\n",
    "    # test phase\n",
    "    num_epochs = 1\n",
    "    for model_class in model_classes:\n",
    "        for crf_type in crf_types:\n",
    "            if('Pair' in model_class):\n",
    "                prefix = 'crf{}pair'.format(crf_type)\n",
    "            else:\n",
    "                prefix = 'crf{}'.format(crf_type)\n",
    "            trainmodel_dir = os.path.join(project_dir, 'train_models', prefix)\n",
    "            target_dir = create_directory('{}_train'.format(prefix), trainmodel_dir)\n",
    "            testmodel_dir = os.path.join(project_dir, 'test_models', prefix)\n",
    "            wrk_dir = create_directory('{}_test'.format(prefix), testmodel_dir)\n",
    "            scoredict = train_eval_models(target_dir, wrk_dir, model_class, y_codebook, num_epochs,\n",
    "                                          to_gpu, memmap, dataset_dir,'test',\n",
    "                                         {'reg_type':'l2',\n",
    "                                          'decoder_type':decoder_type,\n",
    "                                          'crf_type': '{}_crf'.format(crf_type),\n",
    "                                          'bidirection':bidirection,\n",
    "                                          'dec_outdir':create_directory('{}_decoded'.format(prefix), decoded_dir)})\n",
    "            print(\"testing:\")\n",
    "            print(scoredict)\n",
    "            print(\"-\"*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
