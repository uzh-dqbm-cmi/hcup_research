{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we define relevant directories\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "# project directory\n",
    "project_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "# src directory (below)\n",
    "src_dir = os.path.join(project_dir, 'src')\n",
    "sys.path.insert(0, src_dir)\n",
    "dataset_dir = os.path.join(project_dir, \"dataset\")\n",
    "print(\"dataset_dir: \", dataset_dir)\n",
    "print(\"project_dir: \", project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from explore_hcupdata import *\n",
    "from utilities import create_directory, ReaderWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read relevant data\n",
    "CONT_COLS = ReaderWriter.read_data(os.path.join(dataset_dir, 'continuous_features.pkl'))\n",
    "COL_FEATURES = ReaderWriter.read_data(os.path.join(dataset_dir, 'col_features.pkl'))\n",
    "feat_label = ReaderWriter.read_data(os.path.join(dataset_dir, 'feat_label.pkl'))\n",
    "code_feat = ReaderWriter.read_data(os.path.join(dataset_dir, 'code_feat.pkl'))\n",
    "datasplit = ReaderWriter.read_data(os.path.join(dataset_dir, 'datasplit.pkl'))\n",
    "traj_info = ReaderWriter.read_data(os.path.join(dataset_dir, 'traj_info.pkl'))\n",
    "fsample = ReaderWriter.read_data(os.path.join(dataset_dir, 'fsample.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate baseline model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score, roc_auc_score, \\\n",
    "                            brier_score_loss, average_precision_score\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "def results_report(y_ref, y_pred, pred_prob, clf_name, dump_file=None):\n",
    "    lsep = \"\\n\"\n",
    "    report = \"Classification report on last event:\" + lsep\n",
    "    report += str(classification_report(y_ref, y_pred)) + lsep\n",
    "    report += \"weighted f1:\" + lsep\n",
    "    weighted_f1 = f1_score(y_ref, y_pred, average='weighted')\n",
    "    report += str(weighted_f1) + lsep\n",
    "    report += \"micro f1:\" + lsep\n",
    "    report += str(f1_score(y_ref, y_pred, average='micro')) + lsep\n",
    "    report += \"brier score:\" + lsep\n",
    "    brierscore = brier_score_loss(y_ref, pred_prob, pos_label=y_ref.max())\n",
    "    report += str(brierscore) + lsep\n",
    "    avg_precrecall = average_precision_score(y_ref, y_pred, average='micro')\n",
    "    report += \"average precision recall, method={}:\".format('micro') + lsep\n",
    "    report += str(avg_precrecall) + lsep\n",
    "    report += \"auc:\" + lsep\n",
    "    auc_score = roc_auc_score(y_ref, pred_prob)\n",
    "    report += str(auc_score) + lsep\n",
    "    report += \"-\"*30 + lsep + \"*\"*30 + lsep\n",
    "    print(report)\n",
    "    if(dump_file):\n",
    "        score = (weighted_f1, auc_score)\n",
    "        ReaderWriter.dump_data(score, dump_file)\n",
    "    return(auc_score)\n",
    "                                       \n",
    "def collapse_avgfeature(gdf, target_outcome):\n",
    "    avgfeat = gdf[COL_FEATURES].mean(axis=0)\n",
    "    avgfeat[target_outcome] = gdf.iloc[-1][target_outcome]\n",
    "    return(avgfeat)\n",
    "\n",
    "def construct_data_baseline(target_idx, fsample, target_outcome='allcause_readmit', collapse_option='last'):\n",
    "    dset = fsample.loc[fsample['nrd_visitlink'].isin(target_idx)].copy()\n",
    "    if(collapse_option=='last'):\n",
    "        dset = dset.groupby('nrd_visitlink', group_keys=False).nth(-1)\n",
    "        dset.reset_index(inplace=True)\n",
    "    elif(collapse_option=='average'):\n",
    "        dset = dset.groupby('nrd_visitlink', group_keys=False).apply(collapse_avgfeature, target_outcome)\n",
    "        dset.reset_index(inplace=True)\n",
    "    return(dset)\n",
    "\n",
    "def generate_baseline_normalizer(dset, norm_option, cdir):\n",
    "    print(\"norm_option: \", norm_option)\n",
    "    print(cdir)\n",
    "    if(norm_option=='standardize'):\n",
    "        normalizer = GaussianNormalizerInfo\n",
    "    elif(norm_option=='meanrange'):\n",
    "        normalizer = MeanRangeNormalizerInfo\n",
    "    elif(norm_option == 'rescale'):\n",
    "        normalizer = RescaleNormalizerInfo\n",
    "    a, b = get_feature_normalizer(dset, CONT_COLS, norm_option)\n",
    "    ReaderWriter.dump_data(normalizer(a,b), os.path.join(cdir, (\"{}_info.pkl\".format(norm_option))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_baseline_models(datafolds, norm_option, collapse_option, clf_name, reg_type,\n",
    "                               dataset_dir, wrkdir, dec_dir, target_outcome = 'allcause_readmit'):\n",
    "    dsettypes = ('train', 'validation', 'test')\n",
    "    for fold_name in datafolds:\n",
    "        print(\"fold name: \", fold_name)\n",
    "        # model signature/name\n",
    "        sign = \"{}_{}_{}_{}.pkl\".format(fold_name, norm_option, collapse_option, clf_name)\n",
    "        print(\"model signature: \", sign)\n",
    "        # dataset directory\n",
    "        cdir = create_directory(\"{}_{}_{}\".format(fold_name, norm_option, collapse_option), dataset_dir) \n",
    "        fitmodels = []\n",
    "        fitmodels_map = {}\n",
    "        # datafolds[fold_name] is a tuple of (train, validation, test) \n",
    "        for i, dset in enumerate(datafolds[fold_name]):\n",
    "            dsettype =dsettypes[i]\n",
    "            print(dsettype + \" dataset\")\n",
    "            print(\"dset shape: \", dset.shape)\n",
    "            print()\n",
    "            if(norm_option != 'none'): # apply normalization for continuous features\n",
    "                apply_normalization(dset, \n",
    "                                    CONT_COLS, \n",
    "                                    ReaderWriter.read_data(os.path.join(cdir, \"{}_info.pkl\".format(norm_option))))   \n",
    "            if(dsettype == 'train'):\n",
    "                for solver in ('liblinear', 'saga'):\n",
    "                    for C in (1e-1, 1e-2, 1e-3):\n",
    "                        for class_weight in ('balanced',):\n",
    "                            model_spec = \"solver:{}, reg_type:{}, C:{}, class_weight:{}\".format(solver,\n",
    "                                                                                                reg_type,\n",
    "                                                                                                C,\n",
    "                                                                                                class_weight)\n",
    "                            print(\"model_spec: \", model_spec)                                \n",
    "                            lr = LogisticRegression(solver=solver, penalty=reg_type, \n",
    "                                                    class_weight=class_weight, C=C)\n",
    "                            lr.fit(dset[COL_FEATURES], dset[target_outcome])\n",
    "                            fitmodels.append(lr)\n",
    "                            fitmodels_map[len(fitmodels_map)] = model_spec\n",
    "            elif(dsettype == 'validation'):\n",
    "                res = []\n",
    "                for model in fitmodels:\n",
    "                    y_pred = model.predict(dset[COL_FEATURES])\n",
    "                    y_ref = dset[target_outcome]\n",
    "                    pred_prob = model.predict_proba(dset[COL_FEATURES])[:, 1]                          \n",
    "                    score = results_report(y_ref, y_pred, pred_prob, 'logistic regression')\n",
    "                    res.append(score)\n",
    "                maxarg = np.argmax(res)                \n",
    "                trainedmodel = fitmodels[maxarg]\n",
    "                print(\"trainedmodel: \", trainedmodel)\n",
    "                print(\"bestmodel signature: \", fitmodels_map[maxarg])\n",
    "                model_name = str(trainedmodel.__class__).split('.')[-1][:-2]\n",
    "                ReaderWriter.dump_data(trainedmodel, os.path.join(wrkdir, \"{}_{}_{}_{}.pkl\".format(model_name,\n",
    "                                                                                                   collapse_option,\n",
    "                                                                                                   fold_name,\n",
    "                                                                                                   norm_option)))\n",
    "                score = res[maxarg]\n",
    "            elif(dsettype == 'test'):\n",
    "                y_pred = trainedmodel.predict(dset[COL_FEATURES])\n",
    "                y_ref = dset[target_outcome]\n",
    "                pred_prob = trainedmodel.predict_proba(dset[COL_FEATURES])[:, 1]\n",
    "                dump_file = os.path.join(wrkdir, \"{}_{}_{}_{}_{}_score.pkl\".format(model_name,\n",
    "                                                                                   collapse_option,\n",
    "                                                                                   fold_name,\n",
    "                                                                                   norm_option,\n",
    "                                                                                   dsettype))\n",
    "                score = results_report(y_ref, y_pred, pred_prob, 'logistic regression', dump_file=dump_file)\n",
    "                # dump dataframe to disk\n",
    "                df = dset[['nrd_visitlink', 'seq_len', 'index_event', target_outcome]].copy()\n",
    "                df['pred_target'] = y_pred\n",
    "                df['prob_target1'] = pred_prob\n",
    "                df['model_name'] = clf_name\n",
    "                df['fold_id'] = \"{}_{}\".format(fold_name, norm_option)\n",
    "                df.rename(index=str, columns={\"nrd_visitlink\": \"pid\", target_outcome:'ref_target'}, inplace=True)\n",
    "                fpath = os.path.join(dec_dir,\"{}_{}_{}_{}.txt\".format(clf_name,\n",
    "                                                                      collapse_option,\n",
    "                                                                      fold_name,\n",
    "                                                                      norm_option))\n",
    "                dump_df(df, fpath, sep=\"\\t\")\n",
    "    return(score)\n",
    "\n",
    "def dump_df(df, fpath, sep=\"\\t\"):\n",
    "    f_out = open(fpath, 'a')\n",
    "    f_out.write(sep.join(df.columns.tolist()) + \"\\n\")\n",
    "    f_out.close()\n",
    "    df.to_csv(fpath, mode='a', index=False, header=False, sep=sep, na_rep='NaN')\n",
    "    \n",
    "# we can merge this function with the :func:`train_eval_baseline_models`\n",
    "def test_baseline_models(datafolds, norm_option, collapse_option, clf_name, \n",
    "                         dataset_dir, model_dir, dec_dir, target_outcome = 'allcause_readmit'):\n",
    "    dsettypes = ('test',)\n",
    "    for fold_name in datafolds:\n",
    "        print(\"fold name: \", fold_name)\n",
    "        sign = \"{}_{}_{}_{}.pkl\".format(fold_name, norm_option, collapse_option, clf_name)\n",
    "        print(\"model signature: \", sign)\n",
    "        # dataset directory\n",
    "        cdir = create_directory(\"{}_{}_{}\".format(fold_name, norm_option, collapse_option), dataset_dir)\n",
    "        # datafolds[fold_name] is a tuple of (train, validation, test)\n",
    "        for i, dset in enumerate(datafolds[fold_name]): \n",
    "            dsettype =dsettypes[i]\n",
    "            print(dsettype + \" dataset\")\n",
    "            print(\"dset shape: \", dset.shape)\n",
    "            print()\n",
    "            # read model\n",
    "            trainedmodel = ReaderWriter.read_data(os.path.join(model_dir, \"{}_{}_{}_{}.pkl\".format(clf_name,\n",
    "                                                                                                   collapse_option\n",
    "                                                                                                   fold_name,\n",
    "                                                                                                   norm_option)))\n",
    "            y_pred = trainedmodel.predict(dset[COL_FEATURES])\n",
    "            y_ref = dset[target_outcome]\n",
    "            pred_prob = trainedmodel.predict_proba(dset[COL_FEATURES])[:, 1]\n",
    "            dump_file = os.path.join(model_dir, \"{}_{}_{}_{}_{}_score.pkl\".format(clf_name,\n",
    "                                                                                  collapse_option,\n",
    "                                                                                  fold_name,\n",
    "                                                                                  norm_option,\n",
    "                                                                                  dsettype))\n",
    "            score = results_report(y_ref, y_pred, pred_prob, 'logistic regression', dump_file=dump_file)\n",
    "            # dump dataframe to disk\n",
    "            df = dset[['nrd_visitlink', 'seq_len', 'index_event', target_outcome]].copy()\n",
    "            df['pred_target'] = y_pred\n",
    "            df['prob_target1'] = pred_prob\n",
    "            df['model_name'] = clf_name\n",
    "            df['fold_id'] = \"{}_{}\".format(fold_name, norm_option)\n",
    "            df.rename(index=str, columns={\"nrd_visitlink\": \"pid\", target_outcome:'ref_target'}, inplace=True)\n",
    "            fpath = os.path.join(dec_dir, \"{}_{}_{}_{}.txt\".format(clf_name,\n",
    "                                                                   collapse_option,\n",
    "                                                                   fold_name,\n",
    "                                                                   norm_option))\n",
    "            dump_df(df, fpath, sep=\"\\t\")\n",
    "    return(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate dataset for training/testing baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_dataset_dir = create_directory('dataset_baseline', project_dir)\n",
    "decoded_dir = create_directory('decoded_output', project_dir)\n",
    "# datafolds -> fold_id:(train_idx, val_idx, test_idx)\n",
    "datafolds = get_datafolds(datasplit, traj_info, 0.2)\n",
    "norm_option = 'none'\n",
    "collapse_option = 'last'\n",
    "dsettypes = ['train', 'validation', 'test']\n",
    "target_outcome = 'allcause_readmit'\n",
    "for fold_name in datafolds:\n",
    "    dirname = \"{}_{}_{}\".format(fold_name, norm_option, collapse_option)\n",
    "    cdir = create_directory(dirname, baseline_dataset_dir)\n",
    "    for i, target_idx in enumerate(datafolds[fold_name]):\n",
    "        dset = construct_data_baseline(target_idx, fsample, \n",
    "                                       target_outcome = target_outcome,\n",
    "                                       collapse_option=collapse_option)\n",
    "        dsettype = dsettypes[i]\n",
    "        # save to disk\n",
    "        dset.to_pickle(os.path.join(cdir, \"{}.pkl\".format(dsettype)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train models with l1 and l2 regularization\n",
    "clf_name = 'LogisticRegression'\n",
    "res = {}\n",
    "wrkdir_f = create_directory('models_baseline', project_dir)\n",
    "fold_names =  [\"fold_{}\".format(i) for i in range(0, 5)]\n",
    "for reg_type in ('l1', 'l2'):\n",
    "    print(\"{} with regularization type: {}\".format(clf_name, reg_type))\n",
    "    wrkdir = create_directory('{}_{}'.format(clf_name, reg_type), wrkdir_f)\n",
    "    dec_dir = create_directory('{}_{}_decoded'.format(clf_name, reg_type), decoded_dir)\n",
    "    for fold_name in fold_names:\n",
    "        folds = {fold_name: []}\n",
    "        dirname = \"{}_{}_{}\".format(fold_name, norm_option, collapse_option)\n",
    "        cdir = create_directory(dirname, baseline_dataset_dir) \n",
    "        for dsettype in dsettypes:\n",
    "            dset = ReaderWriter.read_data(os.path.join(cdir, \"{}.pkl\".format(dsettype)))\n",
    "            folds[fold_name].append(dset)\n",
    "        res[fold_name] = train_eval_baseline_models(folds, norm_option, collapse_option, clf_name, reg_type,\n",
    "                                                    baseline_dataset_dir, wrkdir, dec_dir, \n",
    "                                                    target_outcome=target_outcome)\n",
    "    print(\"results:\\n\", res)\n",
    "    print(\"-\"*35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
