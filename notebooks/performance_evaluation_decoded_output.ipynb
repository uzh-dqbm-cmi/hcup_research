{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we define relevant directories\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "# project directory\n",
    "project_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report, f1_score, roc_auc_score, roc_curve, \\\n",
    "                            precision_recall_curve, average_precision_score, confusion_matrix\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_perf_df(gdf):\n",
    "    ref_target_levent = gdf['ref_target']\n",
    "    pred_target_levent = gdf['pred_target']\n",
    "    prob_score_levent = gdf['prob_target1']\n",
    "    lsep = \"\\n\"\n",
    "    report = \"Classification report on last event:\" + lsep\n",
    "    report += str(classification_report(ref_target_levent, pred_target_levent)) + lsep\n",
    "    micro_f1 = None\n",
    "    for method in ('binary','micro', 'macro', 'weighted'):\n",
    "        f_score = f1_score(ref_target_levent, pred_target_levent, average=method)\n",
    "        report += \"{} f1:\".format(method) + lsep\n",
    "        report += str(f_score) + lsep\n",
    "        if(method=='micro'):\n",
    "            micro_f1 = f_score\n",
    "    report += str(confusion_matrix(ref_target_levent, pred_target_levent)) + lsep\n",
    "    report += \"auc:\" + lsep\n",
    "    try:\n",
    "        auc_score = roc_auc_score(ref_target_levent, prob_score_levent)\n",
    "    except Exception:\n",
    "        print(\"exception is raised !!\")\n",
    "        auc_score = 0\n",
    "    finally:\n",
    "        report += str(auc_score) + lsep\n",
    "        for method in ('micro',):\n",
    "            avg_precrecall = average_precision_score(ref_target_levent, pred_target_levent, average=method)\n",
    "            report += \"average precision recall, method={}:\".format(method) + lsep\n",
    "            report += str(avg_precrecall) + lsep\n",
    "        report += \"-\"*30 + lsep + \"*\"*30 + lsep\n",
    "        return(micro_f1, auc_score, report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 5\n",
    "model_desc = [('cnn', 'CNN_Labeler'),\n",
    "              ('cnnwide', 'CNNWide_Labeler'),\n",
    "              ('nn','NN_Labeler'),\n",
    "              ('LogisticRegression_l1', 'LogisticRegression_last'),\n",
    "              ('LogisticRegression_l2', 'LogisticRegression_last'),\n",
    "              ('crfnn', 'CRF_NN_Labeler'),\n",
    "              ('crfnnpair', 'CRF_NN_Pair_Labeler'),\n",
    "              ('crfonly','CRF_Only_Labeler'),\n",
    "              ('crfonlypair','CRF_Only_Pair_Labeler'),\n",
    "              ('rnncrfpair', 'RNNCRF_Pair_Labeler'),\n",
    "              ('rnncrfunary', 'RNNCRF_Unary_Labeler')]\n",
    "rnn_losses = ('Convex_HF_LastHF', 'LastHF', 'Uniform_HF', 'Convex_HF_NonHF')\n",
    "for m in ('rnn', 'rnnss'):\n",
    "    for mloss in rnn_losses:\n",
    "        model_desc += [(\"{}_lossmode_{}\".format(m, mloss), \"{}_Labeler_lossmode_{}\".format(m.upper(), mloss))]\n",
    "data_dir = os.path.join(project_dir, 'decoded_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sanity check\n",
    "for folder_name, model_prefix in model_desc:\n",
    "    print(\"model name: \", model_prefix)\n",
    "    for fold in range(num_folds):\n",
    "        print('fold num: ', fold)\n",
    "        print()\n",
    "        df = pd.read_csv(os.path.join(data_dir, \"{}_decoded\".format(folder_name), \"{}_fold_{}_none.txt\".format(model_prefix, fold)), header=0, sep=\"\\t\")\n",
    "        if(pd.isnull(df['prob_target1']).sum()):\n",
    "            print(\"*\"*15)\n",
    "            print(\"NULL found!!!\")\n",
    "            print(\"folder name: \", folder_name)\n",
    "            print(\"fold num: \", fold)\n",
    "            print(\"*\"*15)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update each decoded text to make all files in same representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder_name, model_prefix in model_desc:\n",
    "    print(\"model name: \", model_prefix)\n",
    "    for fold in range(num_folds):\n",
    "        print('fold num: ', fold)\n",
    "        print()\n",
    "        df = pd.read_csv(os.path.join(data_dir, \"{}_decoded\".format(folder_name), \"{}_fold_{}_none.txt\".format(model_prefix, fold)), header=0, sep=\"\\t\")\n",
    "        df['rindx'] = df.index.tolist()\n",
    "        df['lastevent'] = 0\n",
    "        lastevent_indx = df.groupby('pid').nth(-1)['rindx'].tolist()\n",
    "        df.loc[df['rindx'].isin(lastevent_indx), 'lastevent']=1\n",
    "        df.to_csv(os.path.join(data_dir, \"{}_decoded\".format(folder_name), \"{}_fold_{}_none_upd.txt\".format(model_prefix, fold)), sep=\"\\t\", header=True, index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute performance of each fold\n",
    "cols = ['micro f1', 'AUC']\n",
    "verbose = True\n",
    "for folder_name, model_prefix in model_desc:\n",
    "    res_levent = np.zeros((5,2))\n",
    "    res_index_wo_levent = np.zeros((5,2))\n",
    "    print(\"folder name: \", folder_name)\n",
    "    print(\"model name: \", model_prefix)\n",
    "    flag = False\n",
    "    for fold in range(num_folds):\n",
    "        print('fold num: ', fold)\n",
    "        print()\n",
    "        report = ''\n",
    "        if(folder_name not in {'cnn','cnnwide','nn', 'LogisticRegression_l1', 'LogisticRegression_l2'}):\n",
    "            flag = True\n",
    "            df = pd.read_csv(os.path.join(data_dir, \"{}_decoded\".format(folder_name), \"{}_fold_{}_none_upd.txt\".format(model_prefix, fold)), \n",
    "                             sep=\"\\t\", header=0)  \n",
    "            report += \"evaluating performance using only last index event: \\n\"\n",
    "            tmp = df.loc[(df['index_event']==1) & (df['lastevent']==1)]\n",
    "            micro_f1, auc, eval_rep = eval_perf_df(tmp)\n",
    "            res_levent[fold,:] = np.array([micro_f1, auc])\n",
    "            report += eval_rep\n",
    "\n",
    "#             report += \"evaluating performance using all index events not including the last event: \\n\"\n",
    "#             tmp = df.loc[(df['index_event']==1) & (df['lastevent']!=1)]\n",
    "#             micro_f1, auc, eval_rep= eval_perf_df(tmp)\n",
    "#             res_index_wo_levent[fold,:] = np.array([micro_f1, auc])\n",
    "#             report += eval_rep\n",
    "\n",
    "#             report += \"evaluating performance using all index events (including the last event)\\n\"\n",
    "#             tmp = df.loc[(df['index_event']==1)]\n",
    "#             _, _, eval_rep = eval_perf_df(tmp)\n",
    "#             report += eval_rep\n",
    "\n",
    "#             report += \"evaluating performance using all events (including non index events)\\n\"\n",
    "#             _, _, eval_rep = eval_perf_df(df)\n",
    "#             report += eval_rep\n",
    "        else:\n",
    "            df = pd.read_csv(os.path.join(data_dir, \"{}_decoded\".format(folder_name), \"{}_fold_{}_none.txt\".format(model_prefix, fold)), \n",
    "                             sep=\"\\t\", header=0)\n",
    "            report += \"evaluating performance using only last index event: \\n\"\n",
    "            micro_f1, auc, eval_rep = eval_perf_df(df)\n",
    "            res_levent[fold,:] = np.array([micro_f1, auc])\n",
    "            report += eval_rep\n",
    "        if(verbose):\n",
    "            print(report)\n",
    "    print(\"|\"*100)\n",
    "    print()\n",
    "    for i in range(2):\n",
    "        print(\"average performance (across five folds) using only last index event:\")\n",
    "        print(\"{} mean:{} sd:{}\".format(cols[i], np.mean(res_levent[:,i]), np.std(res_levent[:,i])))\n",
    "        print()\n",
    "        if(flag):\n",
    "            print(\"average performance (across five folds) using all index events not including the last event:\")\n",
    "            print(\"{} mean:{} sd:{}\".format(cols[i], np.mean(res_index_wo_levent[:,i]), np.std(res_index_wo_levent[:,i])))\n",
    "            print()\n",
    "    print(\"|\"*100)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_perf_df(gdf, i):\n",
    "    ref_target_levent = gdf['ref_target']\n",
    "    pred_target_levent = gdf['pred_target']\n",
    "    prob_score_levent = gdf['prob_target1']\n",
    "    try:\n",
    "        auc_score = roc_auc_score(ref_target_levent, prob_score_levent)\n",
    "    except Exception:\n",
    "        print(\"exception is raised !!\")\n",
    "        auc_score = 0\n",
    "    finally:\n",
    "        df = pd.DataFrame()\n",
    "        df['seq_len'] = [i]\n",
    "        df['auc'] = [auc_score]\n",
    "        df['fold_id'] = [gdf.iloc[-1]['fold_id']]\n",
    "        df['model_name'] = [gdf.iloc[-1]['model_name']]\n",
    "        return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame()\n",
    "seqs_len = list(range(1,11)) + [20]\n",
    "for folder_name, model_prefix in model_desc:\n",
    "    print(\"folder name: \", folder_name)\n",
    "    print(\"model name: \", model_prefix)\n",
    "    for fold in range(num_folds):\n",
    "        print('fold num: ', fold)\n",
    "        print()\n",
    "        df = pd.read_csv(os.path.join(data_dir, \"{}_decoded\".format(folder_name), \"{}_fold_{}_none_upd.txt\".format(model_prefix, fold)), \n",
    "                         sep=\"\\t\", header=0)\n",
    "        if(folder_name not in {'cnn','cnnwide','nn', 'LogisticRegression_l1', 'LogisticRegression_l2'}):\n",
    "            df = df.loc[(df['index_event']==1) & (df['lastevent']==1)].copy() # use only last event\n",
    "        df['seq_len_categ'] = df['seq_len']\n",
    "        for seq_len in seqs_len:\n",
    "            if(seq_len == 20):\n",
    "                seq_len = 11\n",
    "            tmp = df.loc[df['seq_len'] <= seq_len].copy()\n",
    "            auc_df = generate_perf_df(tmp, seq_len)\n",
    "            final_df = pd.concat([final_df, auc_df], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale = 1.4)\n",
    "sns.set_style('white')\n",
    "g = sns.factorplot(x=\"seq_len\", y='auc', hue='model_name',\n",
    "                   data=final_df.loc[final_df['model_name'].isin({'LogisticRegression_l1','RNNCRF_Pair_Labeler'})],\n",
    "                   size=8, aspect=1);\n",
    "g.set_ylabels('AUC', fontsize = 20)\n",
    "g.set_xlabels(\"Patients' timeline length\", fontsize=20)\n",
    "g.set_xticklabels(['<= {}'.format(i) for i in list(range(1,11)) + [20]], fontsize=18)\n",
    "\n",
    "# title\n",
    "new_title = 'Model name'\n",
    "g._legend.set_title(\"\")\n",
    "# replace labels\n",
    "new_labels = ['LASSO', 'RNNCRF (Pairwise potential)']\n",
    "for t, l in zip(g._legend.texts, new_labels): t.set_text(l); t.set_fontsize(18);\n",
    "\n",
    "sns.plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.savefig(os.path.join(project_dir, 'performance_vs_seqlen.svg'),format='svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the decoded output of both LASSO and RNNCRF Pairwise potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# traj_info = pd.read_csv(os.path.join(project_dir, 'dataset', 'traj_info.txt'), header=0, sep=\"\\t\")\n",
    "# model_a = ('LogisticRegression_l1', 'LogisticRegression_last')\n",
    "# model_b = ('rnncrfpair', 'RNNCRF_Pair_Labeler')\n",
    "# for fold_num in range(5):\n",
    "#     print(\"Fold number: \", fold_num)\n",
    "#     df_a = pd.read_csv(os.path.join(data_dir, \"{}_decoded\".format(model_a[0]), \"{}_fold_{}_none_upd.txt\".format(model_a[-1], fold)), \n",
    "#                        sep=\"\\t\", header=0)  \n",
    "#     df_b = pd.read_csv(os.path.join(data_dir, \"{}_decoded\".format(model_b[0]), \"{}_fold_{}_none_upd.txt\".format(model_b[-1], fold)), \n",
    "#                        sep=\"\\t\", header=0)  \n",
    "#     df_b = df_b.loc[(df_b['index_event']==1) & (df_b['lastevent']==1)].copy()\n",
    "#     # correct prediction users for RNNCRF Pair model\n",
    "#     setcorrect_b = set(df_b.loc[df_b['ref_target'] == df_b['pred_target'], 'pid'])\n",
    "#     # correct prediction users for Logistic regression model\n",
    "#     setcorrect_a = set(df_a.loc[df_a['ref_target'] == df_a['pred_target'], 'pid'])\n",
    "    \n",
    "#     print(\"Number of users correctly predicted outcomes by RNNCRF model and not by Logistic regression: \\n\", \n",
    "#           len(setcorrect_b - setcorrect_a))\n",
    "#     print(\"Number of users correctly predicted outcomes by Logistic regression and not by RNNCRF model: \\n\", \n",
    "#           len(setcorrect_a - setcorrect_b))\n",
    "#     print(\"Number of users correctly predicted outcomes by both models: \\n\", \n",
    "#           len(setcorrect_b.intersection(setcorrect_a)))\n",
    "#     print()\n",
    "#     b_minus_a = setcorrect_b - setcorrect_a\n",
    "#     print('Characteristics of users correctly predicted by RNNCRF model and not by Logistic regression:')\n",
    "#     print(traj_info.loc[traj_info['nrd_visitlink'].isin(b_minus_a), ['female', 'age', \n",
    "#                                                                      'run_num_indxevents', 'seq_len',\n",
    "#                                                                      'count_allcausereadmit']].describe())\n",
    "#     print()\n",
    "#     print(\"percentage of females:\")\n",
    "#     print(traj_info.loc[traj_info['nrd_visitlink'].isin(b_minus_a), 'female'].value_counts(normalize=True))\n",
    "#     print()\n",
    "#     print(\"percentage of allcause readmissions:\")\n",
    "#     print(traj_info.loc[traj_info['nrd_visitlink'].isin(b_minus_a), 'allcause_readmit'].value_counts(normalize=True))\n",
    "#     print()\n",
    "#     a_minus_b = setcorrect_a - setcorrect_b\n",
    "#     print('Characteristics of users correctly predicted by Logistic regression and not by RNNCRF model:')\n",
    "#     print(traj_info.loc[traj_info['nrd_visitlink'].isin(a_minus_b), ['female', 'age', \n",
    "#                                                                      'run_num_indxevents', 'seq_len',\n",
    "#                                                                      'count_allcausereadmit']].describe())\n",
    "#     print()\n",
    "#     print(\"percentage of females:\")\n",
    "#     print(traj_info.loc[traj_info['nrd_visitlink'].isin(a_minus_b), 'female'].value_counts(normalize=True))\n",
    "#     print()\n",
    "#     print(\"percentage of allcause readmissions:\")\n",
    "#     print(traj_info.loc[traj_info['nrd_visitlink'].isin(a_minus_b), 'allcause_readmit'].value_counts(normalize=True))\n",
    "#     print()\n",
    "#     print(\"*\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
